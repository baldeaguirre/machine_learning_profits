{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "choice-prince",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geographic-facing",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.DMatrix # core data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tender-traffic",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.sklearn.XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "typical-honolulu",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class DMatrix in module xgboost.core:\n",
      "\n",
      "class DMatrix(builtins.object)\n",
      " |  DMatrix(data, label=None, weight=None, base_margin=None, missing=None, silent=False, feature_names=None, feature_types=None, nthread=None, enable_categorical=False)\n",
      " |  \n",
      " |  Data Matrix used in XGBoost.\n",
      " |  \n",
      " |  DMatrix is an internal data structure that is used by XGBoost,\n",
      " |  which is optimized for both memory efficiency and training speed.\n",
      " |  You can construct DMatrix from multiple different sources of data.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __del__(self)\n",
      " |  \n",
      " |  __init__(self, data, label=None, weight=None, base_margin=None, missing=None, silent=False, feature_names=None, feature_types=None, nthread=None, enable_categorical=False)\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : os.PathLike/string/numpy.array/scipy.sparse/pd.DataFrame/\n",
      " |             dt.Frame/cudf.DataFrame/cupy.array/dlpack\n",
      " |          Data source of DMatrix.\n",
      " |          When data is string or os.PathLike type, it represents the path\n",
      " |          libsvm format txt file, csv file (by specifying uri parameter\n",
      " |          'path_to_csv?format=csv'), or binary file that xgboost can read\n",
      " |          from.\n",
      " |      label : list, numpy 1-D array or cudf.DataFrame, optional\n",
      " |          Label of the training data.\n",
      " |      missing : float, optional\n",
      " |          Value in the input data which needs to be present as a missing\n",
      " |          value. If None, defaults to np.nan.\n",
      " |      weight : list, numpy 1-D array or cudf.DataFrame , optional\n",
      " |          Weight for each instance.\n",
      " |      \n",
      " |          .. note:: For ranking task, weights are per-group.\n",
      " |      \n",
      " |              In ranking task, one weight is assigned to each group (not each\n",
      " |              data point). This is because we only care about the relative\n",
      " |              ordering of data points within each group, so it doesn't make\n",
      " |              sense to assign weights to individual data points.\n",
      " |      \n",
      " |      silent : boolean, optional\n",
      " |          Whether print messages during construction\n",
      " |      feature_names : list, optional\n",
      " |          Set names for features.\n",
      " |      feature_types : list, optional\n",
      " |          Set types for features.\n",
      " |      nthread : integer, optional\n",
      " |          Number of threads to use for loading data when parallelization is\n",
      " |          applicable. If -1, uses maximum threads available on the system.\n",
      " |      \n",
      " |      enable_categorical: boolean, optional\n",
      " |      \n",
      " |          .. versionadded:: 1.3.0\n",
      " |      \n",
      " |          Experimental support of specializing for categorical features.  Do\n",
      " |          not set to True unless you are interested in development.\n",
      " |          Currently it's only available for `gpu_hist` tree method with 1 vs\n",
      " |          rest (one hot) categorical split.  Also, JSON serialization format,\n",
      " |          `gpu_predictor` and pandas input are required.\n",
      " |  \n",
      " |  get_base_margin(self)\n",
      " |      Get the base margin of the DMatrix.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      base_margin : float\n",
      " |  \n",
      " |  get_float_info(self, field)\n",
      " |      Get float property from the DMatrix.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      field: str\n",
      " |          The field name of the information\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      info : array\n",
      " |          a numpy array of float information of the data\n",
      " |  \n",
      " |  get_label(self)\n",
      " |      Get the label of the DMatrix.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      label : array\n",
      " |  \n",
      " |  get_uint_info(self, field)\n",
      " |      Get unsigned integer property from the DMatrix.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      field: str\n",
      " |          The field name of the information\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      info : array\n",
      " |          a numpy array of unsigned integer information of the data\n",
      " |  \n",
      " |  get_weight(self)\n",
      " |      Get the weight of the DMatrix.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      weight : array\n",
      " |  \n",
      " |  num_col(self)\n",
      " |      Get the number of columns (features) in the DMatrix.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      number of columns : int\n",
      " |  \n",
      " |  num_row(self)\n",
      " |      Get the number of rows in the DMatrix.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      number of rows : int\n",
      " |  \n",
      " |  save_binary(self, fname, silent=True)\n",
      " |      Save DMatrix to an XGBoost buffer.  Saved binary can be later loaded\n",
      " |      by providing the path to :py:func:`xgboost.DMatrix` as input.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : string or os.PathLike\n",
      " |          Name of the output buffer file.\n",
      " |      silent : bool (optional; default: True)\n",
      " |          If set, the output is suppressed.\n",
      " |  \n",
      " |  set_base_margin(self, margin)\n",
      " |      Set base margin of booster to start from.\n",
      " |      \n",
      " |      This can be used to specify a prediction value of existing model to be\n",
      " |      base_margin However, remember margin is needed, instead of transformed\n",
      " |      prediction e.g. for logistic regression: need to put in value before\n",
      " |      logistic transformation see also example/demo.py\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      margin: array like\n",
      " |          Prediction margin of each datapoint\n",
      " |  \n",
      " |  set_float_info(self, field, data)\n",
      " |      Set float type property into the DMatrix.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      field: str\n",
      " |          The field name of the information\n",
      " |      \n",
      " |      data: numpy array\n",
      " |          The array of data to be set\n",
      " |  \n",
      " |  set_float_info_npy2d(self, field, data)\n",
      " |      Set float type property into the DMatrix\n",
      " |         for numpy 2d array input\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      field: str\n",
      " |          The field name of the information\n",
      " |      \n",
      " |      data: numpy array\n",
      " |          The array of data to be set\n",
      " |  \n",
      " |  set_group(self, group)\n",
      " |      Set group size of DMatrix (used for ranking).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      group : array like\n",
      " |          Group size of each group\n",
      " |  \n",
      " |  set_info(self, *, label=None, weight=None, base_margin=None, group=None, label_lower_bound=None, label_upper_bound=None, feature_names=None, feature_types=None, feature_weights=None)\n",
      " |      Set meta info for DMatrix.\n",
      " |  \n",
      " |  set_label(self, label)\n",
      " |      Set label of dmatrix\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      label: array like\n",
      " |          The label information to be set into DMatrix\n",
      " |  \n",
      " |  set_uint_info(self, field, data)\n",
      " |      Set uint type property into the DMatrix.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      field: str\n",
      " |          The field name of the information\n",
      " |      \n",
      " |      data: numpy array\n",
      " |          The array of data to be set\n",
      " |  \n",
      " |  set_weight(self, weight)\n",
      " |      Set weight of each instance.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      weight : array like\n",
      " |          Weight for each data point\n",
      " |      \n",
      " |          .. note:: For ranking task, weights are per-group.\n",
      " |      \n",
      " |              In ranking task, one weight is assigned to each group (not each\n",
      " |              data point). This is because we only care about the relative\n",
      " |              ordering of data points within each group, so it doesn't make\n",
      " |              sense to assign weights to individual data points.\n",
      " |  \n",
      " |  slice(self, rindex, allow_groups=False)\n",
      " |      Slice the DMatrix and return a new DMatrix that only contains `rindex`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      rindex : list\n",
      " |          List of indices to be selected.\n",
      " |      allow_groups : boolean\n",
      " |          Allow slicing of a matrix with a groups attribute\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      res : DMatrix\n",
      " |          A new DMatrix containing only selected indices.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  feature_names\n",
      " |      Get feature names (column labels).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_names : list or None\n",
      " |  \n",
      " |  feature_types\n",
      " |      Get feature types (column types).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_types : list or None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(xgb.DMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "thorough-library",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module xgboost.sklearn in xgboost:\n",
      "\n",
      "NAME\n",
      "    xgboost.sklearn - Scikit-Learn Wrapper interface for XGBoost.\n",
      "\n",
      "CLASSES\n",
      "    sklearn.base.BaseEstimator(builtins.object)\n",
      "        XGBModel\n",
      "            XGBClassifier(XGBModel, sklearn.base.ClassifierMixin)\n",
      "                XGBRFClassifier\n",
      "            XGBRanker\n",
      "            XGBRegressor(XGBModel, sklearn.base.RegressorMixin)\n",
      "                XGBRFRegressor\n",
      "    \n",
      "    class XGBClassifier(XGBModel, sklearn.base.ClassifierMixin)\n",
      "     |  XGBClassifier(*, objective='binary:logistic', use_label_encoder=True, **kwargs)\n",
      "     |  \n",
      "     |  Implementation of the scikit-learn API for XGBoost classification.\n",
      "     |  \n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  \n",
      "     |      n_estimators : int\n",
      "     |          Number of boosting rounds.\n",
      "     |      use_label_encoder : bool\n",
      "     |          (Deprecated) Use the label encoder from scikit-learn to encode the labels. For new code,\n",
      "     |          we recommend that you set this parameter to False.\n",
      "     |  \n",
      "     |      max_depth : int\n",
      "     |          Maximum tree depth for base learners.\n",
      "     |      learning_rate : float\n",
      "     |          Boosting learning rate (xgb's \"eta\")\n",
      "     |      verbosity : int\n",
      "     |          The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
      "     |      objective : string or callable\n",
      "     |          Specify the learning task and the corresponding learning objective or\n",
      "     |          a custom objective function to be used (see note below).\n",
      "     |      booster: string\n",
      "     |          Specify which booster to use: gbtree, gblinear or dart.\n",
      "     |      tree_method: string\n",
      "     |          Specify which tree method to use.  Default to auto.  If this parameter\n",
      "     |          is set to default, XGBoost will choose the most conservative option\n",
      "     |          available.  It's recommended to study this option from parameters\n",
      "     |          document.\n",
      "     |      n_jobs : int\n",
      "     |          Number of parallel threads used to run xgboost.  When used with other Scikit-Learn\n",
      "     |          algorithms like grid search, you may choose which algorithm to parallelize and\n",
      "     |          balance the threads.  Creating thread contention will significantly slow dowm both\n",
      "     |          algorithms.\n",
      "     |      gamma : float\n",
      "     |          Minimum loss reduction required to make a further partition on a leaf\n",
      "     |          node of the tree.\n",
      "     |      min_child_weight : float\n",
      "     |          Minimum sum of instance weight(hessian) needed in a child.\n",
      "     |      max_delta_step : int\n",
      "     |          Maximum delta step we allow each tree's weight estimation to be.\n",
      "     |      subsample : float\n",
      "     |          Subsample ratio of the training instance.\n",
      "     |      colsample_bytree : float\n",
      "     |          Subsample ratio of columns when constructing each tree.\n",
      "     |      colsample_bylevel : float\n",
      "     |          Subsample ratio of columns for each level.\n",
      "     |      colsample_bynode : float\n",
      "     |          Subsample ratio of columns for each split.\n",
      "     |      reg_alpha : float (xgb's alpha)\n",
      "     |          L1 regularization term on weights\n",
      "     |      reg_lambda : float (xgb's lambda)\n",
      "     |          L2 regularization term on weights\n",
      "     |      scale_pos_weight : float\n",
      "     |          Balancing of positive and negative weights.\n",
      "     |      base_score:\n",
      "     |          The initial prediction score of all instances, global bias.\n",
      "     |      random_state : int\n",
      "     |          Random number seed.\n",
      "     |  \n",
      "     |          .. note::\n",
      "     |  \n",
      "     |             Using gblinear booster with shotgun updater is nondeterministic as\n",
      "     |             it uses Hogwild algorithm.\n",
      "     |  \n",
      "     |      missing : float, default np.nan\n",
      "     |          Value in the data which needs to be present as a missing value.\n",
      "     |      num_parallel_tree: int\n",
      "     |          Used for boosting random forest.\n",
      "     |      monotone_constraints : str\n",
      "     |          Constraint of variable monotonicity.  See tutorial for more\n",
      "     |          information.\n",
      "     |      interaction_constraints : str\n",
      "     |          Constraints for interaction representing permitted interactions.  The\n",
      "     |          constraints must be specified in the form of a nest list, e.g. [[0, 1],\n",
      "     |          [2, 3, 4]], where each inner list is a group of indices of features\n",
      "     |          that are allowed to interact with each other.  See tutorial for more\n",
      "     |          information\n",
      "     |      importance_type: string, default \"gain\"\n",
      "     |          The feature importance type for the feature_importances\\_ property:\n",
      "     |          either \"gain\", \"weight\", \"cover\", \"total_gain\" or \"total_cover\".\n",
      "     |  \n",
      "     |      \\*\\*kwargs : dict, optional\n",
      "     |          Keyword arguments for XGBoost Booster object.  Full documentation of\n",
      "     |          parameters can be found here:\n",
      "     |          https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst.\n",
      "     |          Attempting to set a parameter via the constructor args and \\*\\*kwargs\n",
      "     |          dict simultaneously will result in a TypeError.\n",
      "     |  \n",
      "     |          .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
      "     |  \n",
      "     |              \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee\n",
      "     |              that parameters passed via this argument will interact properly\n",
      "     |              with scikit-learn.\n",
      "     |  \n",
      "     |          .. note::  Custom objective function\n",
      "     |  \n",
      "     |              A custom objective function can be provided for the ``objective``\n",
      "     |              parameter. In this case, it should have the signature\n",
      "     |              ``objective(y_true, y_pred) -> grad, hess``:\n",
      "     |  \n",
      "     |              y_true: array_like of shape [n_samples]\n",
      "     |                  The target values\n",
      "     |              y_pred: array_like of shape [n_samples]\n",
      "     |                  The predicted values\n",
      "     |  \n",
      "     |              grad: array_like of shape [n_samples]\n",
      "     |                  The value of the gradient for each sample point.\n",
      "     |              hess: array_like of shape [n_samples]\n",
      "     |                  The value of the second derivative for each sample point\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      XGBClassifier\n",
      "     |      XGBModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, objective='binary:logistic', use_label_encoder=True, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  evals_result(self)\n",
      "     |      Return the evaluation results.\n",
      "     |      \n",
      "     |      If **eval_set** is passed to the `fit` function, you can call\n",
      "     |      ``evals_result()`` to get evaluation results for all passed **eval_sets**.\n",
      "     |      When **eval_metric** is also passed to the `fit` function, the\n",
      "     |      **evals_result** will contain the **eval_metrics** passed to the `fit` function.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      evals_result : dictionary\n",
      "     |      \n",
      "     |      Example\n",
      "     |      -------\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          param_dist = {'objective':'binary:logistic', 'n_estimators':2}\n",
      "     |      \n",
      "     |          clf = xgb.XGBClassifier(**param_dist)\n",
      "     |      \n",
      "     |          clf.fit(X_train, y_train,\n",
      "     |                  eval_set=[(X_train, y_train), (X_test, y_test)],\n",
      "     |                  eval_metric='logloss',\n",
      "     |                  verbose=True)\n",
      "     |      \n",
      "     |          evals_result = clf.evals_result()\n",
      "     |      \n",
      "     |      The variable **evals_result** will contain\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
      "     |          'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
      "     |  \n",
      "     |  fit(self, X, y, *, sample_weight=None, base_margin=None, eval_set=None, eval_metric=None, early_stopping_rounds=None, verbose=True, xgb_model=None, sample_weight_eval_set=None, feature_weights=None, callbacks=None)\n",
      "     |      Fit gradient boosting classifier\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like\n",
      "     |          Feature matrix\n",
      "     |      y : array_like\n",
      "     |          Labels\n",
      "     |      sample_weight : array_like\n",
      "     |          instance weights\n",
      "     |      base_margin : array_like\n",
      "     |          global bias for each instance.\n",
      "     |      eval_set : list, optional\n",
      "     |          A list of (X, y) tuple pairs to use as validation sets, for which\n",
      "     |          metrics will be computed.\n",
      "     |          Validation metrics will help us track the performance of the model.\n",
      "     |      eval_metric : str, list of str, or callable, optional\n",
      "     |          If a str, should be a built-in evaluation metric to use. See\n",
      "     |          doc/parameter.rst.\n",
      "     |          If a list of str, should be the list of multiple built-in evaluation metrics\n",
      "     |          to use.\n",
      "     |          If callable, a custom evaluation metric. The call\n",
      "     |          signature is ``func(y_predicted, y_true)`` where ``y_true`` will be a\n",
      "     |          DMatrix object such that you may need to call the ``get_label``\n",
      "     |          method. It must return a str, value pair where the str is a name\n",
      "     |          for the evaluation and value is the value of the evaluation\n",
      "     |          function. The callable custom objective is always minimized.\n",
      "     |      early_stopping_rounds : int\n",
      "     |          Activates early stopping. Validation metric needs to improve at least once in\n",
      "     |          every **early_stopping_rounds** round(s) to continue training.\n",
      "     |          Requires at least one item in **eval_set**.\n",
      "     |          The method returns the model from the last iteration (not the best one).\n",
      "     |          If there's more than one item in **eval_set**, the last entry will be used\n",
      "     |          for early stopping.\n",
      "     |          If there's more than one metric in **eval_metric**, the last metric will be\n",
      "     |          used for early stopping.\n",
      "     |          If early stopping occurs, the model will have three additional fields:\n",
      "     |          ``clf.best_score``, ``clf.best_iteration`` and ``clf.best_ntree_limit``.\n",
      "     |      verbose : bool\n",
      "     |          If `verbose` and an evaluation set is used, writes the evaluation\n",
      "     |          metric measured on the validation set to stderr.\n",
      "     |      xgb_model : str\n",
      "     |          file name of stored XGBoost model or 'Booster' instance XGBoost model to be\n",
      "     |          loaded before training (allows training continuation).\n",
      "     |      sample_weight_eval_set : list, optional\n",
      "     |          A list of the form [L_1, L_2, ..., L_n], where each L_i is a list of\n",
      "     |          instance weights on the i-th validation set.\n",
      "     |      feature_weights: array_like\n",
      "     |          Weight for each feature, defines the probability of each feature being\n",
      "     |          selected when colsample is being used.  All values must be greater than 0,\n",
      "     |          otherwise a `ValueError` is thrown.  Only available for `hist`, `gpu_hist` and\n",
      "     |          `exact` tree methods.\n",
      "     |      callbacks : list of callback functions\n",
      "     |          List of callback functions that are applied at end of each iteration.\n",
      "     |          It is possible to use predefined callbacks by using :ref:`callback_api`.\n",
      "     |          Example:\n",
      "     |      \n",
      "     |          .. code-block:: python\n",
      "     |      \n",
      "     |              callbacks = [xgb.callback.EarlyStopping(rounds=early_stopping_rounds,\n",
      "     |                                                      save_best=True)]\n",
      "     |  \n",
      "     |  predict(self, data, output_margin=False, ntree_limit=None, validate_features=True, base_margin=None)\n",
      "     |      Predict with `data`.\n",
      "     |      \n",
      "     |      .. note:: This function is not thread safe.\n",
      "     |      \n",
      "     |        For each booster object, predict can only be called from one thread.\n",
      "     |        If you want to run prediction using multiple thread, call\n",
      "     |        ``xgb.copy()`` to make copies of model object and then call\n",
      "     |        ``predict()``.\n",
      "     |      \n",
      "     |        .. code-block:: python\n",
      "     |      \n",
      "     |          preds = bst.predict(dtest, ntree_limit=num_round)\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : array_like\n",
      "     |          Feature matrix.\n",
      "     |      output_margin : bool\n",
      "     |          Whether to output the raw untransformed margin value.\n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to\n",
      "     |          best_ntree_limit if defined (i.e. it has been trained with early\n",
      "     |          stopping), otherwise 0 (use all trees).\n",
      "     |      validate_features : bool\n",
      "     |          When this is True, validate that the Booster's and data's\n",
      "     |          feature_names are identical.  Otherwise, it is assumed that the\n",
      "     |          feature_names are the same.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      prediction : numpy array\n",
      "     |  \n",
      "     |  predict_proba(self, X, ntree_limit=None, validate_features=False, base_margin=None)\n",
      "     |      Predict the probability of each `X` example being of a given class.\n",
      "     |      \n",
      "     |      .. note:: This function is not thread safe\n",
      "     |      \n",
      "     |          For each booster object, predict can only be called from one\n",
      "     |          thread.  If you want to run prediction using multiple thread, call\n",
      "     |          ``xgb.copy()`` to make copies of model object and then call predict\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like\n",
      "     |          Feature matrix.\n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to best_ntree_limit if\n",
      "     |          defined (i.e. it has been trained with early stopping), otherwise 0 (use all\n",
      "     |          trees).\n",
      "     |      validate_features : bool\n",
      "     |          When this is True, validate that the Booster's and data's feature_names are\n",
      "     |          identical.  Otherwise, it is assumed that the feature_names are the same.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      prediction : numpy array\n",
      "     |          a numpy array with the probability of each data example being of a given class.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from XGBModel:\n",
      "     |  \n",
      "     |  apply(self, X, ntree_limit=0)\n",
      "     |      Return the predicted leaf every tree for each sample.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like, shape=[n_samples, n_features]\n",
      "     |          Input features matrix.\n",
      "     |      \n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to 0 (use all trees).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array_like, shape=[n_samples, n_trees]\n",
      "     |          For each datapoint x in X and for each tree, return the index of the\n",
      "     |          leaf x ends up in. Leaves are numbered within\n",
      "     |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
      "     |  \n",
      "     |  get_booster(self)\n",
      "     |      Get the underlying xgboost Booster of this model.\n",
      "     |      \n",
      "     |      This will raise an exception when fit was not called\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      booster : a xgboost booster of underlying model\n",
      "     |  \n",
      "     |  get_num_boosting_rounds(self)\n",
      "     |      Gets the number of xgboost boosting rounds.\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters.\n",
      "     |  \n",
      "     |  get_xgb_params(self)\n",
      "     |      Get xgboost specific parameters.\n",
      "     |  \n",
      "     |  load_model(self, fname)\n",
      "     |      Load the model from a file.\n",
      "     |      \n",
      "     |      The model is loaded from an XGBoost internal format which is universal\n",
      "     |      among the various XGBoost interfaces. Auxiliary attributes of the\n",
      "     |      Python Booster object (such as feature names) will not be loaded.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string\n",
      "     |          Input file name.\n",
      "     |  \n",
      "     |  save_model(self, fname: str)\n",
      "     |      Save the model to a file.\n",
      "     |      \n",
      "     |      The model is saved in an XGBoost internal format which is universal\n",
      "     |      among the various XGBoost interfaces. Auxiliary attributes of the\n",
      "     |      Python Booster object (such as feature names) will not be saved.\n",
      "     |      \n",
      "     |        .. note::\n",
      "     |      \n",
      "     |          See:\n",
      "     |      \n",
      "     |          https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string\n",
      "     |          Output file name\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.  Modification of the sklearn method to\n",
      "     |      allow unknown kwargs. This allows using the full range of xgboost\n",
      "     |      parameters that are not defined as member variables in sklearn grid\n",
      "     |      search.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from XGBModel:\n",
      "     |  \n",
      "     |  coef_\n",
      "     |      Coefficients property\n",
      "     |      \n",
      "     |      .. note:: Coefficients are defined only for linear learners\n",
      "     |      \n",
      "     |          Coefficients are only defined when the linear model is chosen as\n",
      "     |          base learner (`booster=gblinear`). It is not defined for other base\n",
      "     |          learner types, such as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      coef_ : array of shape ``[n_features]`` or ``[n_classes, n_features]``\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Feature importances property\n",
      "     |      \n",
      "     |      .. note:: Feature importance is defined only for tree boosters\n",
      "     |      \n",
      "     |          Feature importance is only defined when the decision tree model is chosen as base\n",
      "     |          learner (`booster=gbtree`). It is not defined for other base learner types, such\n",
      "     |          as linear learners (`booster=gblinear`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : array of shape ``[n_features]``\n",
      "     |  \n",
      "     |  intercept_\n",
      "     |      Intercept (bias) property\n",
      "     |      \n",
      "     |      .. note:: Intercept is defined only for linear learners\n",
      "     |      \n",
      "     |          Intercept (bias) is only defined when the linear model is chosen as base\n",
      "     |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      "     |          as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      intercept_ : array of shape ``(1,)`` or ``[n_classes]``\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "    \n",
      "    class XGBModel(sklearn.base.BaseEstimator)\n",
      "     |  XGBModel(max_depth=None, learning_rate=None, n_estimators=100, verbosity=None, objective=None, booster=None, tree_method=None, n_jobs=None, gamma=None, min_child_weight=None, max_delta_step=None, subsample=None, colsample_bytree=None, colsample_bylevel=None, colsample_bynode=None, reg_alpha=None, reg_lambda=None, scale_pos_weight=None, base_score=None, random_state=None, missing=nan, num_parallel_tree=None, monotone_constraints=None, interaction_constraints=None, importance_type='gain', gpu_id=None, validate_parameters=None, **kwargs)\n",
      "     |  \n",
      "     |  Implementation of the Scikit-Learn API for XGBoost.\n",
      "     |  \n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  \n",
      "     |      n_estimators : int\n",
      "     |          Number of gradient boosted trees.  Equivalent to number of boosting\n",
      "     |          rounds.\n",
      "     |  \n",
      "     |      max_depth : int\n",
      "     |          Maximum tree depth for base learners.\n",
      "     |      learning_rate : float\n",
      "     |          Boosting learning rate (xgb's \"eta\")\n",
      "     |      verbosity : int\n",
      "     |          The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
      "     |      objective : string or callable\n",
      "     |          Specify the learning task and the corresponding learning objective or\n",
      "     |          a custom objective function to be used (see note below).\n",
      "     |      booster: string\n",
      "     |          Specify which booster to use: gbtree, gblinear or dart.\n",
      "     |      tree_method: string\n",
      "     |          Specify which tree method to use.  Default to auto.  If this parameter\n",
      "     |          is set to default, XGBoost will choose the most conservative option\n",
      "     |          available.  It's recommended to study this option from parameters\n",
      "     |          document.\n",
      "     |      n_jobs : int\n",
      "     |          Number of parallel threads used to run xgboost.  When used with other Scikit-Learn\n",
      "     |          algorithms like grid search, you may choose which algorithm to parallelize and\n",
      "     |          balance the threads.  Creating thread contention will significantly slow dowm both\n",
      "     |          algorithms.\n",
      "     |      gamma : float\n",
      "     |          Minimum loss reduction required to make a further partition on a leaf\n",
      "     |          node of the tree.\n",
      "     |      min_child_weight : float\n",
      "     |          Minimum sum of instance weight(hessian) needed in a child.\n",
      "     |      max_delta_step : int\n",
      "     |          Maximum delta step we allow each tree's weight estimation to be.\n",
      "     |      subsample : float\n",
      "     |          Subsample ratio of the training instance.\n",
      "     |      colsample_bytree : float\n",
      "     |          Subsample ratio of columns when constructing each tree.\n",
      "     |      colsample_bylevel : float\n",
      "     |          Subsample ratio of columns for each level.\n",
      "     |      colsample_bynode : float\n",
      "     |          Subsample ratio of columns for each split.\n",
      "     |      reg_alpha : float (xgb's alpha)\n",
      "     |          L1 regularization term on weights\n",
      "     |      reg_lambda : float (xgb's lambda)\n",
      "     |          L2 regularization term on weights\n",
      "     |      scale_pos_weight : float\n",
      "     |          Balancing of positive and negative weights.\n",
      "     |      base_score:\n",
      "     |          The initial prediction score of all instances, global bias.\n",
      "     |      random_state : int\n",
      "     |          Random number seed.\n",
      "     |  \n",
      "     |          .. note::\n",
      "     |  \n",
      "     |             Using gblinear booster with shotgun updater is nondeterministic as\n",
      "     |             it uses Hogwild algorithm.\n",
      "     |  \n",
      "     |      missing : float, default np.nan\n",
      "     |          Value in the data which needs to be present as a missing value.\n",
      "     |      num_parallel_tree: int\n",
      "     |          Used for boosting random forest.\n",
      "     |      monotone_constraints : str\n",
      "     |          Constraint of variable monotonicity.  See tutorial for more\n",
      "     |          information.\n",
      "     |      interaction_constraints : str\n",
      "     |          Constraints for interaction representing permitted interactions.  The\n",
      "     |          constraints must be specified in the form of a nest list, e.g. [[0, 1],\n",
      "     |          [2, 3, 4]], where each inner list is a group of indices of features\n",
      "     |          that are allowed to interact with each other.  See tutorial for more\n",
      "     |          information\n",
      "     |      importance_type: string, default \"gain\"\n",
      "     |          The feature importance type for the feature_importances\\_ property:\n",
      "     |          either \"gain\", \"weight\", \"cover\", \"total_gain\" or \"total_cover\".\n",
      "     |  \n",
      "     |      \\*\\*kwargs : dict, optional\n",
      "     |          Keyword arguments for XGBoost Booster object.  Full documentation of\n",
      "     |          parameters can be found here:\n",
      "     |          https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst.\n",
      "     |          Attempting to set a parameter via the constructor args and \\*\\*kwargs\n",
      "     |          dict simultaneously will result in a TypeError.\n",
      "     |  \n",
      "     |          .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
      "     |  \n",
      "     |              \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee\n",
      "     |              that parameters passed via this argument will interact properly\n",
      "     |              with scikit-learn.\n",
      "     |  \n",
      "     |          .. note::  Custom objective function\n",
      "     |  \n",
      "     |              A custom objective function can be provided for the ``objective``\n",
      "     |              parameter. In this case, it should have the signature\n",
      "     |              ``objective(y_true, y_pred) -> grad, hess``:\n",
      "     |  \n",
      "     |              y_true: array_like of shape [n_samples]\n",
      "     |                  The target values\n",
      "     |              y_pred: array_like of shape [n_samples]\n",
      "     |                  The predicted values\n",
      "     |  \n",
      "     |              grad: array_like of shape [n_samples]\n",
      "     |                  The value of the gradient for each sample point.\n",
      "     |              hess: array_like of shape [n_samples]\n",
      "     |                  The value of the second derivative for each sample point\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      XGBModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, max_depth=None, learning_rate=None, n_estimators=100, verbosity=None, objective=None, booster=None, tree_method=None, n_jobs=None, gamma=None, min_child_weight=None, max_delta_step=None, subsample=None, colsample_bytree=None, colsample_bylevel=None, colsample_bynode=None, reg_alpha=None, reg_lambda=None, scale_pos_weight=None, base_score=None, random_state=None, missing=nan, num_parallel_tree=None, monotone_constraints=None, interaction_constraints=None, importance_type='gain', gpu_id=None, validate_parameters=None, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  apply(self, X, ntree_limit=0)\n",
      "     |      Return the predicted leaf every tree for each sample.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like, shape=[n_samples, n_features]\n",
      "     |          Input features matrix.\n",
      "     |      \n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to 0 (use all trees).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array_like, shape=[n_samples, n_trees]\n",
      "     |          For each datapoint x in X and for each tree, return the index of the\n",
      "     |          leaf x ends up in. Leaves are numbered within\n",
      "     |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
      "     |  \n",
      "     |  evals_result(self)\n",
      "     |      Return the evaluation results.\n",
      "     |      \n",
      "     |      If **eval_set** is passed to the `fit` function, you can call\n",
      "     |      ``evals_result()`` to get evaluation results for all passed **eval_sets**.\n",
      "     |      When **eval_metric** is also passed to the `fit` function, the\n",
      "     |      **evals_result** will contain the **eval_metrics** passed to the `fit` function.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      evals_result : dictionary\n",
      "     |      \n",
      "     |      Example\n",
      "     |      -------\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          param_dist = {'objective':'binary:logistic', 'n_estimators':2}\n",
      "     |      \n",
      "     |          clf = xgb.XGBModel(**param_dist)\n",
      "     |      \n",
      "     |          clf.fit(X_train, y_train,\n",
      "     |                  eval_set=[(X_train, y_train), (X_test, y_test)],\n",
      "     |                  eval_metric='logloss',\n",
      "     |                  verbose=True)\n",
      "     |      \n",
      "     |          evals_result = clf.evals_result()\n",
      "     |      \n",
      "     |      The variable **evals_result** will contain:\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
      "     |          'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
      "     |  \n",
      "     |  fit(self, X, y, *, sample_weight=None, base_margin=None, eval_set=None, eval_metric=None, early_stopping_rounds=None, verbose=True, xgb_model=None, sample_weight_eval_set=None, feature_weights=None, callbacks=None)\n",
      "     |      Fit gradient boosting model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like\n",
      "     |          Feature matrix\n",
      "     |      y : array_like\n",
      "     |          Labels\n",
      "     |      sample_weight : array_like\n",
      "     |          instance weights\n",
      "     |      base_margin : array_like\n",
      "     |          global bias for each instance.\n",
      "     |      eval_set : list, optional\n",
      "     |          A list of (X, y) tuple pairs to use as validation sets, for which\n",
      "     |          metrics will be computed.\n",
      "     |          Validation metrics will help us track the performance of the model.\n",
      "     |      eval_metric : str, list of str, or callable, optional\n",
      "     |          If a str, should be a built-in evaluation metric to use. See\n",
      "     |          doc/parameter.rst.\n",
      "     |          If a list of str, should be the list of multiple built-in evaluation metrics\n",
      "     |          to use.\n",
      "     |          If callable, a custom evaluation metric. The call\n",
      "     |          signature is ``func(y_predicted, y_true)`` where ``y_true`` will be a\n",
      "     |          DMatrix object such that you may need to call the ``get_label``\n",
      "     |          method. It must return a str, value pair where the str is a name\n",
      "     |          for the evaluation and value is the value of the evaluation\n",
      "     |          function. The callable custom objective is always minimized.\n",
      "     |      early_stopping_rounds : int\n",
      "     |          Activates early stopping. Validation metric needs to improve at least once in\n",
      "     |          every **early_stopping_rounds** round(s) to continue training.\n",
      "     |          Requires at least one item in **eval_set**.\n",
      "     |          The method returns the model from the last iteration (not the best one).\n",
      "     |          If there's more than one item in **eval_set**, the last entry will be used\n",
      "     |          for early stopping.\n",
      "     |          If there's more than one metric in **eval_metric**, the last metric will be\n",
      "     |          used for early stopping.\n",
      "     |          If early stopping occurs, the model will have three additional fields:\n",
      "     |          ``clf.best_score``, ``clf.best_iteration`` and ``clf.best_ntree_limit``.\n",
      "     |      verbose : bool\n",
      "     |          If `verbose` and an evaluation set is used, writes the evaluation\n",
      "     |          metric measured on the validation set to stderr.\n",
      "     |      xgb_model : str\n",
      "     |          file name of stored XGBoost model or 'Booster' instance XGBoost model to be\n",
      "     |          loaded before training (allows training continuation).\n",
      "     |      sample_weight_eval_set : list, optional\n",
      "     |          A list of the form [L_1, L_2, ..., L_n], where each L_i is a list of\n",
      "     |          instance weights on the i-th validation set.\n",
      "     |      feature_weights: array_like\n",
      "     |          Weight for each feature, defines the probability of each feature being\n",
      "     |          selected when colsample is being used.  All values must be greater than 0,\n",
      "     |          otherwise a `ValueError` is thrown.  Only available for `hist`, `gpu_hist` and\n",
      "     |          `exact` tree methods.\n",
      "     |      callbacks : list of callback functions\n",
      "     |          List of callback functions that are applied at end of each iteration.\n",
      "     |          It is possible to use predefined callbacks by using :ref:`callback_api`.\n",
      "     |          Example:\n",
      "     |      \n",
      "     |          .. code-block:: python\n",
      "     |      \n",
      "     |              callbacks = [xgb.callback.EarlyStopping(rounds=early_stopping_rounds,\n",
      "     |                                                      save_best=True)]\n",
      "     |  \n",
      "     |  get_booster(self)\n",
      "     |      Get the underlying xgboost Booster of this model.\n",
      "     |      \n",
      "     |      This will raise an exception when fit was not called\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      booster : a xgboost booster of underlying model\n",
      "     |  \n",
      "     |  get_num_boosting_rounds(self)\n",
      "     |      Gets the number of xgboost boosting rounds.\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters.\n",
      "     |  \n",
      "     |  get_xgb_params(self)\n",
      "     |      Get xgboost specific parameters.\n",
      "     |  \n",
      "     |  load_model(self, fname)\n",
      "     |      Load the model from a file.\n",
      "     |      \n",
      "     |      The model is loaded from an XGBoost internal format which is universal\n",
      "     |      among the various XGBoost interfaces. Auxiliary attributes of the\n",
      "     |      Python Booster object (such as feature names) will not be loaded.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string\n",
      "     |          Input file name.\n",
      "     |  \n",
      "     |  predict(self, data, output_margin=False, ntree_limit=None, validate_features=True, base_margin=None)\n",
      "     |      Predict with `data`.\n",
      "     |      \n",
      "     |      .. note:: This function is not thread safe.\n",
      "     |      \n",
      "     |        For each booster object, predict can only be called from one thread.\n",
      "     |        If you want to run prediction using multiple thread, call ``xgb.copy()`` to make copies\n",
      "     |        of model object and then call ``predict()``.\n",
      "     |      \n",
      "     |        .. code-block:: python\n",
      "     |      \n",
      "     |          preds = bst.predict(dtest, ntree_limit=num_round)\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : numpy.array/scipy.sparse\n",
      "     |          Data to predict with\n",
      "     |      output_margin : bool\n",
      "     |          Whether to output the raw untransformed margin value.\n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to best_ntree_limit if defined\n",
      "     |          (i.e. it has been trained with early stopping), otherwise 0 (use all trees).\n",
      "     |      validate_features : bool\n",
      "     |          When this is True, validate that the Booster's and data's feature_names are identical.\n",
      "     |          Otherwise, it is assumed that the feature_names are the same.\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      prediction : numpy array\n",
      "     |  \n",
      "     |  save_model(self, fname: str)\n",
      "     |      Save the model to a file.\n",
      "     |      \n",
      "     |      The model is saved in an XGBoost internal format which is universal\n",
      "     |      among the various XGBoost interfaces. Auxiliary attributes of the\n",
      "     |      Python Booster object (such as feature names) will not be saved.\n",
      "     |      \n",
      "     |        .. note::\n",
      "     |      \n",
      "     |          See:\n",
      "     |      \n",
      "     |          https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string\n",
      "     |          Output file name\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.  Modification of the sklearn method to\n",
      "     |      allow unknown kwargs. This allows using the full range of xgboost\n",
      "     |      parameters that are not defined as member variables in sklearn grid\n",
      "     |      search.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  coef_\n",
      "     |      Coefficients property\n",
      "     |      \n",
      "     |      .. note:: Coefficients are defined only for linear learners\n",
      "     |      \n",
      "     |          Coefficients are only defined when the linear model is chosen as\n",
      "     |          base learner (`booster=gblinear`). It is not defined for other base\n",
      "     |          learner types, such as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      coef_ : array of shape ``[n_features]`` or ``[n_classes, n_features]``\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Feature importances property\n",
      "     |      \n",
      "     |      .. note:: Feature importance is defined only for tree boosters\n",
      "     |      \n",
      "     |          Feature importance is only defined when the decision tree model is chosen as base\n",
      "     |          learner (`booster=gbtree`). It is not defined for other base learner types, such\n",
      "     |          as linear learners (`booster=gblinear`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : array of shape ``[n_features]``\n",
      "     |  \n",
      "     |  intercept_\n",
      "     |      Intercept (bias) property\n",
      "     |      \n",
      "     |      .. note:: Intercept is defined only for linear learners\n",
      "     |      \n",
      "     |          Intercept (bias) is only defined when the linear model is chosen as base\n",
      "     |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      "     |          as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      intercept_ : array of shape ``(1,)`` or ``[n_classes]``\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class XGBRFClassifier(XGBClassifier)\n",
      "     |  XGBRFClassifier(*, learning_rate=1, subsample=0.8, colsample_bynode=0.8, reg_lambda=1e-05, use_label_encoder=True, **kwargs)\n",
      "     |  \n",
      "     |  scikit-learn API for XGBoost random forest classification.\n",
      "     |  \n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  \n",
      "     |      n_estimators : int\n",
      "     |          Number of trees in random forest to fit.\n",
      "     |      use_label_encoder : bool\n",
      "     |          (Deprecated) Use the label encoder from scikit-learn to encode the labels. For new code,\n",
      "     |          we recommend that you set this parameter to False.\n",
      "     |  \n",
      "     |      max_depth : int\n",
      "     |          Maximum tree depth for base learners.\n",
      "     |      learning_rate : float\n",
      "     |          Boosting learning rate (xgb's \"eta\")\n",
      "     |      verbosity : int\n",
      "     |          The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
      "     |      objective : string or callable\n",
      "     |          Specify the learning task and the corresponding learning objective or\n",
      "     |          a custom objective function to be used (see note below).\n",
      "     |      booster: string\n",
      "     |          Specify which booster to use: gbtree, gblinear or dart.\n",
      "     |      tree_method: string\n",
      "     |          Specify which tree method to use.  Default to auto.  If this parameter\n",
      "     |          is set to default, XGBoost will choose the most conservative option\n",
      "     |          available.  It's recommended to study this option from parameters\n",
      "     |          document.\n",
      "     |      n_jobs : int\n",
      "     |          Number of parallel threads used to run xgboost.  When used with other Scikit-Learn\n",
      "     |          algorithms like grid search, you may choose which algorithm to parallelize and\n",
      "     |          balance the threads.  Creating thread contention will significantly slow dowm both\n",
      "     |          algorithms.\n",
      "     |      gamma : float\n",
      "     |          Minimum loss reduction required to make a further partition on a leaf\n",
      "     |          node of the tree.\n",
      "     |      min_child_weight : float\n",
      "     |          Minimum sum of instance weight(hessian) needed in a child.\n",
      "     |      max_delta_step : int\n",
      "     |          Maximum delta step we allow each tree's weight estimation to be.\n",
      "     |      subsample : float\n",
      "     |          Subsample ratio of the training instance.\n",
      "     |      colsample_bytree : float\n",
      "     |          Subsample ratio of columns when constructing each tree.\n",
      "     |      colsample_bylevel : float\n",
      "     |          Subsample ratio of columns for each level.\n",
      "     |      colsample_bynode : float\n",
      "     |          Subsample ratio of columns for each split.\n",
      "     |      reg_alpha : float (xgb's alpha)\n",
      "     |          L1 regularization term on weights\n",
      "     |      reg_lambda : float (xgb's lambda)\n",
      "     |          L2 regularization term on weights\n",
      "     |      scale_pos_weight : float\n",
      "     |          Balancing of positive and negative weights.\n",
      "     |      base_score:\n",
      "     |          The initial prediction score of all instances, global bias.\n",
      "     |      random_state : int\n",
      "     |          Random number seed.\n",
      "     |  \n",
      "     |          .. note::\n",
      "     |  \n",
      "     |             Using gblinear booster with shotgun updater is nondeterministic as\n",
      "     |             it uses Hogwild algorithm.\n",
      "     |  \n",
      "     |      missing : float, default np.nan\n",
      "     |          Value in the data which needs to be present as a missing value.\n",
      "     |      num_parallel_tree: int\n",
      "     |          Used for boosting random forest.\n",
      "     |      monotone_constraints : str\n",
      "     |          Constraint of variable monotonicity.  See tutorial for more\n",
      "     |          information.\n",
      "     |      interaction_constraints : str\n",
      "     |          Constraints for interaction representing permitted interactions.  The\n",
      "     |          constraints must be specified in the form of a nest list, e.g. [[0, 1],\n",
      "     |          [2, 3, 4]], where each inner list is a group of indices of features\n",
      "     |          that are allowed to interact with each other.  See tutorial for more\n",
      "     |          information\n",
      "     |      importance_type: string, default \"gain\"\n",
      "     |          The feature importance type for the feature_importances\\_ property:\n",
      "     |          either \"gain\", \"weight\", \"cover\", \"total_gain\" or \"total_cover\".\n",
      "     |  \n",
      "     |      \\*\\*kwargs : dict, optional\n",
      "     |          Keyword arguments for XGBoost Booster object.  Full documentation of\n",
      "     |          parameters can be found here:\n",
      "     |          https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst.\n",
      "     |          Attempting to set a parameter via the constructor args and \\*\\*kwargs\n",
      "     |          dict simultaneously will result in a TypeError.\n",
      "     |  \n",
      "     |          .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
      "     |  \n",
      "     |              \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee\n",
      "     |              that parameters passed via this argument will interact properly\n",
      "     |              with scikit-learn.\n",
      "     |  \n",
      "     |          .. note::  Custom objective function\n",
      "     |  \n",
      "     |              A custom objective function can be provided for the ``objective``\n",
      "     |              parameter. In this case, it should have the signature\n",
      "     |              ``objective(y_true, y_pred) -> grad, hess``:\n",
      "     |  \n",
      "     |              y_true: array_like of shape [n_samples]\n",
      "     |                  The target values\n",
      "     |              y_pred: array_like of shape [n_samples]\n",
      "     |                  The predicted values\n",
      "     |  \n",
      "     |              grad: array_like of shape [n_samples]\n",
      "     |                  The value of the gradient for each sample point.\n",
      "     |              hess: array_like of shape [n_samples]\n",
      "     |                  The value of the second derivative for each sample point\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      XGBRFClassifier\n",
      "     |      XGBClassifier\n",
      "     |      XGBModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, learning_rate=1, subsample=0.8, colsample_bynode=0.8, reg_lambda=1e-05, use_label_encoder=True, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_num_boosting_rounds(self)\n",
      "     |      Gets the number of xgboost boosting rounds.\n",
      "     |  \n",
      "     |  get_xgb_params(self)\n",
      "     |      Get xgboost specific parameters.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from XGBClassifier:\n",
      "     |  \n",
      "     |  evals_result(self)\n",
      "     |      Return the evaluation results.\n",
      "     |      \n",
      "     |      If **eval_set** is passed to the `fit` function, you can call\n",
      "     |      ``evals_result()`` to get evaluation results for all passed **eval_sets**.\n",
      "     |      When **eval_metric** is also passed to the `fit` function, the\n",
      "     |      **evals_result** will contain the **eval_metrics** passed to the `fit` function.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      evals_result : dictionary\n",
      "     |      \n",
      "     |      Example\n",
      "     |      -------\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          param_dist = {'objective':'binary:logistic', 'n_estimators':2}\n",
      "     |      \n",
      "     |          clf = xgb.XGBClassifier(**param_dist)\n",
      "     |      \n",
      "     |          clf.fit(X_train, y_train,\n",
      "     |                  eval_set=[(X_train, y_train), (X_test, y_test)],\n",
      "     |                  eval_metric='logloss',\n",
      "     |                  verbose=True)\n",
      "     |      \n",
      "     |          evals_result = clf.evals_result()\n",
      "     |      \n",
      "     |      The variable **evals_result** will contain\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
      "     |          'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
      "     |  \n",
      "     |  fit(self, X, y, *, sample_weight=None, base_margin=None, eval_set=None, eval_metric=None, early_stopping_rounds=None, verbose=True, xgb_model=None, sample_weight_eval_set=None, feature_weights=None, callbacks=None)\n",
      "     |      Fit gradient boosting classifier\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like\n",
      "     |          Feature matrix\n",
      "     |      y : array_like\n",
      "     |          Labels\n",
      "     |      sample_weight : array_like\n",
      "     |          instance weights\n",
      "     |      base_margin : array_like\n",
      "     |          global bias for each instance.\n",
      "     |      eval_set : list, optional\n",
      "     |          A list of (X, y) tuple pairs to use as validation sets, for which\n",
      "     |          metrics will be computed.\n",
      "     |          Validation metrics will help us track the performance of the model.\n",
      "     |      eval_metric : str, list of str, or callable, optional\n",
      "     |          If a str, should be a built-in evaluation metric to use. See\n",
      "     |          doc/parameter.rst.\n",
      "     |          If a list of str, should be the list of multiple built-in evaluation metrics\n",
      "     |          to use.\n",
      "     |          If callable, a custom evaluation metric. The call\n",
      "     |          signature is ``func(y_predicted, y_true)`` where ``y_true`` will be a\n",
      "     |          DMatrix object such that you may need to call the ``get_label``\n",
      "     |          method. It must return a str, value pair where the str is a name\n",
      "     |          for the evaluation and value is the value of the evaluation\n",
      "     |          function. The callable custom objective is always minimized.\n",
      "     |      early_stopping_rounds : int\n",
      "     |          Activates early stopping. Validation metric needs to improve at least once in\n",
      "     |          every **early_stopping_rounds** round(s) to continue training.\n",
      "     |          Requires at least one item in **eval_set**.\n",
      "     |          The method returns the model from the last iteration (not the best one).\n",
      "     |          If there's more than one item in **eval_set**, the last entry will be used\n",
      "     |          for early stopping.\n",
      "     |          If there's more than one metric in **eval_metric**, the last metric will be\n",
      "     |          used for early stopping.\n",
      "     |          If early stopping occurs, the model will have three additional fields:\n",
      "     |          ``clf.best_score``, ``clf.best_iteration`` and ``clf.best_ntree_limit``.\n",
      "     |      verbose : bool\n",
      "     |          If `verbose` and an evaluation set is used, writes the evaluation\n",
      "     |          metric measured on the validation set to stderr.\n",
      "     |      xgb_model : str\n",
      "     |          file name of stored XGBoost model or 'Booster' instance XGBoost model to be\n",
      "     |          loaded before training (allows training continuation).\n",
      "     |      sample_weight_eval_set : list, optional\n",
      "     |          A list of the form [L_1, L_2, ..., L_n], where each L_i is a list of\n",
      "     |          instance weights on the i-th validation set.\n",
      "     |      feature_weights: array_like\n",
      "     |          Weight for each feature, defines the probability of each feature being\n",
      "     |          selected when colsample is being used.  All values must be greater than 0,\n",
      "     |          otherwise a `ValueError` is thrown.  Only available for `hist`, `gpu_hist` and\n",
      "     |          `exact` tree methods.\n",
      "     |      callbacks : list of callback functions\n",
      "     |          List of callback functions that are applied at end of each iteration.\n",
      "     |          It is possible to use predefined callbacks by using :ref:`callback_api`.\n",
      "     |          Example:\n",
      "     |      \n",
      "     |          .. code-block:: python\n",
      "     |      \n",
      "     |              callbacks = [xgb.callback.EarlyStopping(rounds=early_stopping_rounds,\n",
      "     |                                                      save_best=True)]\n",
      "     |  \n",
      "     |  predict(self, data, output_margin=False, ntree_limit=None, validate_features=True, base_margin=None)\n",
      "     |      Predict with `data`.\n",
      "     |      \n",
      "     |      .. note:: This function is not thread safe.\n",
      "     |      \n",
      "     |        For each booster object, predict can only be called from one thread.\n",
      "     |        If you want to run prediction using multiple thread, call\n",
      "     |        ``xgb.copy()`` to make copies of model object and then call\n",
      "     |        ``predict()``.\n",
      "     |      \n",
      "     |        .. code-block:: python\n",
      "     |      \n",
      "     |          preds = bst.predict(dtest, ntree_limit=num_round)\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : array_like\n",
      "     |          Feature matrix.\n",
      "     |      output_margin : bool\n",
      "     |          Whether to output the raw untransformed margin value.\n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to\n",
      "     |          best_ntree_limit if defined (i.e. it has been trained with early\n",
      "     |          stopping), otherwise 0 (use all trees).\n",
      "     |      validate_features : bool\n",
      "     |          When this is True, validate that the Booster's and data's\n",
      "     |          feature_names are identical.  Otherwise, it is assumed that the\n",
      "     |          feature_names are the same.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      prediction : numpy array\n",
      "     |  \n",
      "     |  predict_proba(self, X, ntree_limit=None, validate_features=False, base_margin=None)\n",
      "     |      Predict the probability of each `X` example being of a given class.\n",
      "     |      \n",
      "     |      .. note:: This function is not thread safe\n",
      "     |      \n",
      "     |          For each booster object, predict can only be called from one\n",
      "     |          thread.  If you want to run prediction using multiple thread, call\n",
      "     |          ``xgb.copy()`` to make copies of model object and then call predict\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like\n",
      "     |          Feature matrix.\n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to best_ntree_limit if\n",
      "     |          defined (i.e. it has been trained with early stopping), otherwise 0 (use all\n",
      "     |          trees).\n",
      "     |      validate_features : bool\n",
      "     |          When this is True, validate that the Booster's and data's feature_names are\n",
      "     |          identical.  Otherwise, it is assumed that the feature_names are the same.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      prediction : numpy array\n",
      "     |          a numpy array with the probability of each data example being of a given class.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from XGBModel:\n",
      "     |  \n",
      "     |  apply(self, X, ntree_limit=0)\n",
      "     |      Return the predicted leaf every tree for each sample.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like, shape=[n_samples, n_features]\n",
      "     |          Input features matrix.\n",
      "     |      \n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to 0 (use all trees).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array_like, shape=[n_samples, n_trees]\n",
      "     |          For each datapoint x in X and for each tree, return the index of the\n",
      "     |          leaf x ends up in. Leaves are numbered within\n",
      "     |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
      "     |  \n",
      "     |  get_booster(self)\n",
      "     |      Get the underlying xgboost Booster of this model.\n",
      "     |      \n",
      "     |      This will raise an exception when fit was not called\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      booster : a xgboost booster of underlying model\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters.\n",
      "     |  \n",
      "     |  load_model(self, fname)\n",
      "     |      Load the model from a file.\n",
      "     |      \n",
      "     |      The model is loaded from an XGBoost internal format which is universal\n",
      "     |      among the various XGBoost interfaces. Auxiliary attributes of the\n",
      "     |      Python Booster object (such as feature names) will not be loaded.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string\n",
      "     |          Input file name.\n",
      "     |  \n",
      "     |  save_model(self, fname: str)\n",
      "     |      Save the model to a file.\n",
      "     |      \n",
      "     |      The model is saved in an XGBoost internal format which is universal\n",
      "     |      among the various XGBoost interfaces. Auxiliary attributes of the\n",
      "     |      Python Booster object (such as feature names) will not be saved.\n",
      "     |      \n",
      "     |        .. note::\n",
      "     |      \n",
      "     |          See:\n",
      "     |      \n",
      "     |          https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string\n",
      "     |          Output file name\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.  Modification of the sklearn method to\n",
      "     |      allow unknown kwargs. This allows using the full range of xgboost\n",
      "     |      parameters that are not defined as member variables in sklearn grid\n",
      "     |      search.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from XGBModel:\n",
      "     |  \n",
      "     |  coef_\n",
      "     |      Coefficients property\n",
      "     |      \n",
      "     |      .. note:: Coefficients are defined only for linear learners\n",
      "     |      \n",
      "     |          Coefficients are only defined when the linear model is chosen as\n",
      "     |          base learner (`booster=gblinear`). It is not defined for other base\n",
      "     |          learner types, such as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      coef_ : array of shape ``[n_features]`` or ``[n_classes, n_features]``\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Feature importances property\n",
      "     |      \n",
      "     |      .. note:: Feature importance is defined only for tree boosters\n",
      "     |      \n",
      "     |          Feature importance is only defined when the decision tree model is chosen as base\n",
      "     |          learner (`booster=gbtree`). It is not defined for other base learner types, such\n",
      "     |          as linear learners (`booster=gblinear`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : array of shape ``[n_features]``\n",
      "     |  \n",
      "     |  intercept_\n",
      "     |      Intercept (bias) property\n",
      "     |      \n",
      "     |      .. note:: Intercept is defined only for linear learners\n",
      "     |      \n",
      "     |          Intercept (bias) is only defined when the linear model is chosen as base\n",
      "     |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      "     |          as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      intercept_ : array of shape ``(1,)`` or ``[n_classes]``\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "    \n",
      "    class XGBRFRegressor(XGBRegressor)\n",
      "     |  XGBRFRegressor(*, learning_rate=1, subsample=0.8, colsample_bynode=0.8, reg_lambda=1e-05, **kwargs)\n",
      "     |  \n",
      "     |  scikit-learn API for XGBoost random forest regression.\n",
      "     |  \n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  \n",
      "     |      n_estimators : int\n",
      "     |          Number of trees in random forest to fit.\n",
      "     |  \n",
      "     |      max_depth : int\n",
      "     |          Maximum tree depth for base learners.\n",
      "     |      learning_rate : float\n",
      "     |          Boosting learning rate (xgb's \"eta\")\n",
      "     |      verbosity : int\n",
      "     |          The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
      "     |      objective : string or callable\n",
      "     |          Specify the learning task and the corresponding learning objective or\n",
      "     |          a custom objective function to be used (see note below).\n",
      "     |      booster: string\n",
      "     |          Specify which booster to use: gbtree, gblinear or dart.\n",
      "     |      tree_method: string\n",
      "     |          Specify which tree method to use.  Default to auto.  If this parameter\n",
      "     |          is set to default, XGBoost will choose the most conservative option\n",
      "     |          available.  It's recommended to study this option from parameters\n",
      "     |          document.\n",
      "     |      n_jobs : int\n",
      "     |          Number of parallel threads used to run xgboost.  When used with other Scikit-Learn\n",
      "     |          algorithms like grid search, you may choose which algorithm to parallelize and\n",
      "     |          balance the threads.  Creating thread contention will significantly slow dowm both\n",
      "     |          algorithms.\n",
      "     |      gamma : float\n",
      "     |          Minimum loss reduction required to make a further partition on a leaf\n",
      "     |          node of the tree.\n",
      "     |      min_child_weight : float\n",
      "     |          Minimum sum of instance weight(hessian) needed in a child.\n",
      "     |      max_delta_step : int\n",
      "     |          Maximum delta step we allow each tree's weight estimation to be.\n",
      "     |      subsample : float\n",
      "     |          Subsample ratio of the training instance.\n",
      "     |      colsample_bytree : float\n",
      "     |          Subsample ratio of columns when constructing each tree.\n",
      "     |      colsample_bylevel : float\n",
      "     |          Subsample ratio of columns for each level.\n",
      "     |      colsample_bynode : float\n",
      "     |          Subsample ratio of columns for each split.\n",
      "     |      reg_alpha : float (xgb's alpha)\n",
      "     |          L1 regularization term on weights\n",
      "     |      reg_lambda : float (xgb's lambda)\n",
      "     |          L2 regularization term on weights\n",
      "     |      scale_pos_weight : float\n",
      "     |          Balancing of positive and negative weights.\n",
      "     |      base_score:\n",
      "     |          The initial prediction score of all instances, global bias.\n",
      "     |      random_state : int\n",
      "     |          Random number seed.\n",
      "     |  \n",
      "     |          .. note::\n",
      "     |  \n",
      "     |             Using gblinear booster with shotgun updater is nondeterministic as\n",
      "     |             it uses Hogwild algorithm.\n",
      "     |  \n",
      "     |      missing : float, default np.nan\n",
      "     |          Value in the data which needs to be present as a missing value.\n",
      "     |      num_parallel_tree: int\n",
      "     |          Used for boosting random forest.\n",
      "     |      monotone_constraints : str\n",
      "     |          Constraint of variable monotonicity.  See tutorial for more\n",
      "     |          information.\n",
      "     |      interaction_constraints : str\n",
      "     |          Constraints for interaction representing permitted interactions.  The\n",
      "     |          constraints must be specified in the form of a nest list, e.g. [[0, 1],\n",
      "     |          [2, 3, 4]], where each inner list is a group of indices of features\n",
      "     |          that are allowed to interact with each other.  See tutorial for more\n",
      "     |          information\n",
      "     |      importance_type: string, default \"gain\"\n",
      "     |          The feature importance type for the feature_importances\\_ property:\n",
      "     |          either \"gain\", \"weight\", \"cover\", \"total_gain\" or \"total_cover\".\n",
      "     |  \n",
      "     |      \\*\\*kwargs : dict, optional\n",
      "     |          Keyword arguments for XGBoost Booster object.  Full documentation of\n",
      "     |          parameters can be found here:\n",
      "     |          https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst.\n",
      "     |          Attempting to set a parameter via the constructor args and \\*\\*kwargs\n",
      "     |          dict simultaneously will result in a TypeError.\n",
      "     |  \n",
      "     |          .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
      "     |  \n",
      "     |              \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee\n",
      "     |              that parameters passed via this argument will interact properly\n",
      "     |              with scikit-learn.\n",
      "     |  \n",
      "     |          .. note::  Custom objective function\n",
      "     |  \n",
      "     |              A custom objective function can be provided for the ``objective``\n",
      "     |              parameter. In this case, it should have the signature\n",
      "     |              ``objective(y_true, y_pred) -> grad, hess``:\n",
      "     |  \n",
      "     |              y_true: array_like of shape [n_samples]\n",
      "     |                  The target values\n",
      "     |              y_pred: array_like of shape [n_samples]\n",
      "     |                  The predicted values\n",
      "     |  \n",
      "     |              grad: array_like of shape [n_samples]\n",
      "     |                  The value of the gradient for each sample point.\n",
      "     |              hess: array_like of shape [n_samples]\n",
      "     |                  The value of the second derivative for each sample point\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      XGBRFRegressor\n",
      "     |      XGBRegressor\n",
      "     |      XGBModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, learning_rate=1, subsample=0.8, colsample_bynode=0.8, reg_lambda=1e-05, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_num_boosting_rounds(self)\n",
      "     |      Gets the number of xgboost boosting rounds.\n",
      "     |  \n",
      "     |  get_xgb_params(self)\n",
      "     |      Get xgboost specific parameters.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from XGBModel:\n",
      "     |  \n",
      "     |  apply(self, X, ntree_limit=0)\n",
      "     |      Return the predicted leaf every tree for each sample.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like, shape=[n_samples, n_features]\n",
      "     |          Input features matrix.\n",
      "     |      \n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to 0 (use all trees).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array_like, shape=[n_samples, n_trees]\n",
      "     |          For each datapoint x in X and for each tree, return the index of the\n",
      "     |          leaf x ends up in. Leaves are numbered within\n",
      "     |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
      "     |  \n",
      "     |  evals_result(self)\n",
      "     |      Return the evaluation results.\n",
      "     |      \n",
      "     |      If **eval_set** is passed to the `fit` function, you can call\n",
      "     |      ``evals_result()`` to get evaluation results for all passed **eval_sets**.\n",
      "     |      When **eval_metric** is also passed to the `fit` function, the\n",
      "     |      **evals_result** will contain the **eval_metrics** passed to the `fit` function.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      evals_result : dictionary\n",
      "     |      \n",
      "     |      Example\n",
      "     |      -------\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          param_dist = {'objective':'binary:logistic', 'n_estimators':2}\n",
      "     |      \n",
      "     |          clf = xgb.XGBModel(**param_dist)\n",
      "     |      \n",
      "     |          clf.fit(X_train, y_train,\n",
      "     |                  eval_set=[(X_train, y_train), (X_test, y_test)],\n",
      "     |                  eval_metric='logloss',\n",
      "     |                  verbose=True)\n",
      "     |      \n",
      "     |          evals_result = clf.evals_result()\n",
      "     |      \n",
      "     |      The variable **evals_result** will contain:\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
      "     |          'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
      "     |  \n",
      "     |  fit(self, X, y, *, sample_weight=None, base_margin=None, eval_set=None, eval_metric=None, early_stopping_rounds=None, verbose=True, xgb_model=None, sample_weight_eval_set=None, feature_weights=None, callbacks=None)\n",
      "     |      Fit gradient boosting model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like\n",
      "     |          Feature matrix\n",
      "     |      y : array_like\n",
      "     |          Labels\n",
      "     |      sample_weight : array_like\n",
      "     |          instance weights\n",
      "     |      base_margin : array_like\n",
      "     |          global bias for each instance.\n",
      "     |      eval_set : list, optional\n",
      "     |          A list of (X, y) tuple pairs to use as validation sets, for which\n",
      "     |          metrics will be computed.\n",
      "     |          Validation metrics will help us track the performance of the model.\n",
      "     |      eval_metric : str, list of str, or callable, optional\n",
      "     |          If a str, should be a built-in evaluation metric to use. See\n",
      "     |          doc/parameter.rst.\n",
      "     |          If a list of str, should be the list of multiple built-in evaluation metrics\n",
      "     |          to use.\n",
      "     |          If callable, a custom evaluation metric. The call\n",
      "     |          signature is ``func(y_predicted, y_true)`` where ``y_true`` will be a\n",
      "     |          DMatrix object such that you may need to call the ``get_label``\n",
      "     |          method. It must return a str, value pair where the str is a name\n",
      "     |          for the evaluation and value is the value of the evaluation\n",
      "     |          function. The callable custom objective is always minimized.\n",
      "     |      early_stopping_rounds : int\n",
      "     |          Activates early stopping. Validation metric needs to improve at least once in\n",
      "     |          every **early_stopping_rounds** round(s) to continue training.\n",
      "     |          Requires at least one item in **eval_set**.\n",
      "     |          The method returns the model from the last iteration (not the best one).\n",
      "     |          If there's more than one item in **eval_set**, the last entry will be used\n",
      "     |          for early stopping.\n",
      "     |          If there's more than one metric in **eval_metric**, the last metric will be\n",
      "     |          used for early stopping.\n",
      "     |          If early stopping occurs, the model will have three additional fields:\n",
      "     |          ``clf.best_score``, ``clf.best_iteration`` and ``clf.best_ntree_limit``.\n",
      "     |      verbose : bool\n",
      "     |          If `verbose` and an evaluation set is used, writes the evaluation\n",
      "     |          metric measured on the validation set to stderr.\n",
      "     |      xgb_model : str\n",
      "     |          file name of stored XGBoost model or 'Booster' instance XGBoost model to be\n",
      "     |          loaded before training (allows training continuation).\n",
      "     |      sample_weight_eval_set : list, optional\n",
      "     |          A list of the form [L_1, L_2, ..., L_n], where each L_i is a list of\n",
      "     |          instance weights on the i-th validation set.\n",
      "     |      feature_weights: array_like\n",
      "     |          Weight for each feature, defines the probability of each feature being\n",
      "     |          selected when colsample is being used.  All values must be greater than 0,\n",
      "     |          otherwise a `ValueError` is thrown.  Only available for `hist`, `gpu_hist` and\n",
      "     |          `exact` tree methods.\n",
      "     |      callbacks : list of callback functions\n",
      "     |          List of callback functions that are applied at end of each iteration.\n",
      "     |          It is possible to use predefined callbacks by using :ref:`callback_api`.\n",
      "     |          Example:\n",
      "     |      \n",
      "     |          .. code-block:: python\n",
      "     |      \n",
      "     |              callbacks = [xgb.callback.EarlyStopping(rounds=early_stopping_rounds,\n",
      "     |                                                      save_best=True)]\n",
      "     |  \n",
      "     |  get_booster(self)\n",
      "     |      Get the underlying xgboost Booster of this model.\n",
      "     |      \n",
      "     |      This will raise an exception when fit was not called\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      booster : a xgboost booster of underlying model\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters.\n",
      "     |  \n",
      "     |  load_model(self, fname)\n",
      "     |      Load the model from a file.\n",
      "     |      \n",
      "     |      The model is loaded from an XGBoost internal format which is universal\n",
      "     |      among the various XGBoost interfaces. Auxiliary attributes of the\n",
      "     |      Python Booster object (such as feature names) will not be loaded.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string\n",
      "     |          Input file name.\n",
      "     |  \n",
      "     |  predict(self, data, output_margin=False, ntree_limit=None, validate_features=True, base_margin=None)\n",
      "     |      Predict with `data`.\n",
      "     |      \n",
      "     |      .. note:: This function is not thread safe.\n",
      "     |      \n",
      "     |        For each booster object, predict can only be called from one thread.\n",
      "     |        If you want to run prediction using multiple thread, call ``xgb.copy()`` to make copies\n",
      "     |        of model object and then call ``predict()``.\n",
      "     |      \n",
      "     |        .. code-block:: python\n",
      "     |      \n",
      "     |          preds = bst.predict(dtest, ntree_limit=num_round)\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : numpy.array/scipy.sparse\n",
      "     |          Data to predict with\n",
      "     |      output_margin : bool\n",
      "     |          Whether to output the raw untransformed margin value.\n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to best_ntree_limit if defined\n",
      "     |          (i.e. it has been trained with early stopping), otherwise 0 (use all trees).\n",
      "     |      validate_features : bool\n",
      "     |          When this is True, validate that the Booster's and data's feature_names are identical.\n",
      "     |          Otherwise, it is assumed that the feature_names are the same.\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      prediction : numpy array\n",
      "     |  \n",
      "     |  save_model(self, fname: str)\n",
      "     |      Save the model to a file.\n",
      "     |      \n",
      "     |      The model is saved in an XGBoost internal format which is universal\n",
      "     |      among the various XGBoost interfaces. Auxiliary attributes of the\n",
      "     |      Python Booster object (such as feature names) will not be saved.\n",
      "     |      \n",
      "     |        .. note::\n",
      "     |      \n",
      "     |          See:\n",
      "     |      \n",
      "     |          https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string\n",
      "     |          Output file name\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.  Modification of the sklearn method to\n",
      "     |      allow unknown kwargs. This allows using the full range of xgboost\n",
      "     |      parameters that are not defined as member variables in sklearn grid\n",
      "     |      search.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from XGBModel:\n",
      "     |  \n",
      "     |  coef_\n",
      "     |      Coefficients property\n",
      "     |      \n",
      "     |      .. note:: Coefficients are defined only for linear learners\n",
      "     |      \n",
      "     |          Coefficients are only defined when the linear model is chosen as\n",
      "     |          base learner (`booster=gblinear`). It is not defined for other base\n",
      "     |          learner types, such as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      coef_ : array of shape ``[n_features]`` or ``[n_classes, n_features]``\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Feature importances property\n",
      "     |      \n",
      "     |      .. note:: Feature importance is defined only for tree boosters\n",
      "     |      \n",
      "     |          Feature importance is only defined when the decision tree model is chosen as base\n",
      "     |          learner (`booster=gbtree`). It is not defined for other base learner types, such\n",
      "     |          as linear learners (`booster=gblinear`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : array of shape ``[n_features]``\n",
      "     |  \n",
      "     |  intercept_\n",
      "     |      Intercept (bias) property\n",
      "     |      \n",
      "     |      .. note:: Intercept is defined only for linear learners\n",
      "     |      \n",
      "     |          Intercept (bias) is only defined when the linear model is chosen as base\n",
      "     |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      "     |          as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      intercept_ : array of shape ``(1,)`` or ``[n_classes]``\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a\n",
      "     |          precomputed kernel matrix or a list of generic objects instead,\n",
      "     |          shape = (n_samples, n_samples_fitted),\n",
      "     |          where n_samples_fitted is the number of\n",
      "     |          samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The R2 score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "    \n",
      "    class XGBRanker(XGBModel)\n",
      "     |  XGBRanker(*, objective='rank:pairwise', **kwargs)\n",
      "     |  \n",
      "     |  Implementation of the Scikit-Learn API for XGBoost Ranking.\n",
      "     |  \n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  \n",
      "     |      n_estimators : int\n",
      "     |          Number of gradient boosted trees.  Equivalent to number of boosting\n",
      "     |          rounds.\n",
      "     |  \n",
      "     |      max_depth : int\n",
      "     |          Maximum tree depth for base learners.\n",
      "     |      learning_rate : float\n",
      "     |          Boosting learning rate (xgb's \"eta\")\n",
      "     |      verbosity : int\n",
      "     |          The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
      "     |      objective : string or callable\n",
      "     |          Specify the learning task and the corresponding learning objective or\n",
      "     |          a custom objective function to be used (see note below).\n",
      "     |      booster: string\n",
      "     |          Specify which booster to use: gbtree, gblinear or dart.\n",
      "     |      tree_method: string\n",
      "     |          Specify which tree method to use.  Default to auto.  If this parameter\n",
      "     |          is set to default, XGBoost will choose the most conservative option\n",
      "     |          available.  It's recommended to study this option from parameters\n",
      "     |          document.\n",
      "     |      n_jobs : int\n",
      "     |          Number of parallel threads used to run xgboost.  When used with other Scikit-Learn\n",
      "     |          algorithms like grid search, you may choose which algorithm to parallelize and\n",
      "     |          balance the threads.  Creating thread contention will significantly slow dowm both\n",
      "     |          algorithms.\n",
      "     |      gamma : float\n",
      "     |          Minimum loss reduction required to make a further partition on a leaf\n",
      "     |          node of the tree.\n",
      "     |      min_child_weight : float\n",
      "     |          Minimum sum of instance weight(hessian) needed in a child.\n",
      "     |      max_delta_step : int\n",
      "     |          Maximum delta step we allow each tree's weight estimation to be.\n",
      "     |      subsample : float\n",
      "     |          Subsample ratio of the training instance.\n",
      "     |      colsample_bytree : float\n",
      "     |          Subsample ratio of columns when constructing each tree.\n",
      "     |      colsample_bylevel : float\n",
      "     |          Subsample ratio of columns for each level.\n",
      "     |      colsample_bynode : float\n",
      "     |          Subsample ratio of columns for each split.\n",
      "     |      reg_alpha : float (xgb's alpha)\n",
      "     |          L1 regularization term on weights\n",
      "     |      reg_lambda : float (xgb's lambda)\n",
      "     |          L2 regularization term on weights\n",
      "     |      scale_pos_weight : float\n",
      "     |          Balancing of positive and negative weights.\n",
      "     |      base_score:\n",
      "     |          The initial prediction score of all instances, global bias.\n",
      "     |      random_state : int\n",
      "     |          Random number seed.\n",
      "     |  \n",
      "     |          .. note::\n",
      "     |  \n",
      "     |             Using gblinear booster with shotgun updater is nondeterministic as\n",
      "     |             it uses Hogwild algorithm.\n",
      "     |  \n",
      "     |      missing : float, default np.nan\n",
      "     |          Value in the data which needs to be present as a missing value.\n",
      "     |      num_parallel_tree: int\n",
      "     |          Used for boosting random forest.\n",
      "     |      monotone_constraints : str\n",
      "     |          Constraint of variable monotonicity.  See tutorial for more\n",
      "     |          information.\n",
      "     |      interaction_constraints : str\n",
      "     |          Constraints for interaction representing permitted interactions.  The\n",
      "     |          constraints must be specified in the form of a nest list, e.g. [[0, 1],\n",
      "     |          [2, 3, 4]], where each inner list is a group of indices of features\n",
      "     |          that are allowed to interact with each other.  See tutorial for more\n",
      "     |          information\n",
      "     |      importance_type: string, default \"gain\"\n",
      "     |          The feature importance type for the feature_importances\\_ property:\n",
      "     |          either \"gain\", \"weight\", \"cover\", \"total_gain\" or \"total_cover\".\n",
      "     |  \n",
      "     |      \\*\\*kwargs : dict, optional\n",
      "     |          Keyword arguments for XGBoost Booster object.  Full documentation of\n",
      "     |          parameters can be found here:\n",
      "     |          https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst.\n",
      "     |          Attempting to set a parameter via the constructor args and \\*\\*kwargs\n",
      "     |          dict simultaneously will result in a TypeError.\n",
      "     |  \n",
      "     |          .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
      "     |  \n",
      "     |              \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee\n",
      "     |              that parameters passed via this argument will interact properly\n",
      "     |              with scikit-learn.\n",
      "     |  \n",
      "     |          Note\n",
      "     |          ----\n",
      "     |          A custom objective function is currently not supported by XGBRanker.\n",
      "     |          Likewise, a custom metric function is not supported either.\n",
      "     |  \n",
      "     |          Note\n",
      "     |          ----\n",
      "     |          Query group information is required for ranking tasks.\n",
      "     |  \n",
      "     |          Before fitting the model, your data need to be sorted by query\n",
      "     |          group. When fitting the model, you need to provide an additional array\n",
      "     |          that contains the size of each query group.\n",
      "     |  \n",
      "     |          For example, if your original data look like:\n",
      "     |  \n",
      "     |          +-------+-----------+---------------+\n",
      "     |          |   qid |   label   |   features    |\n",
      "     |          +-------+-----------+---------------+\n",
      "     |          |   1   |   0       |   x_1         |\n",
      "     |          +-------+-----------+---------------+\n",
      "     |          |   1   |   1       |   x_2         |\n",
      "     |          +-------+-----------+---------------+\n",
      "     |          |   1   |   0       |   x_3         |\n",
      "     |          +-------+-----------+---------------+\n",
      "     |          |   2   |   0       |   x_4         |\n",
      "     |          +-------+-----------+---------------+\n",
      "     |          |   2   |   1       |   x_5         |\n",
      "     |          +-------+-----------+---------------+\n",
      "     |          |   2   |   1       |   x_6         |\n",
      "     |          +-------+-----------+---------------+\n",
      "     |          |   2   |   1       |   x_7         |\n",
      "     |          +-------+-----------+---------------+\n",
      "     |  \n",
      "     |          then your group array should be ``[3, 4]``.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      XGBRanker\n",
      "     |      XGBModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, objective='rank:pairwise', **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, *, group, sample_weight=None, base_margin=None, eval_set=None, sample_weight_eval_set=None, eval_group=None, eval_metric=None, early_stopping_rounds=None, verbose=False, xgb_model=None, feature_weights=None, callbacks=None)\n",
      "     |      Fit gradient boosting ranker\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like\n",
      "     |          Feature matrix\n",
      "     |      y : array_like\n",
      "     |          Labels\n",
      "     |      group : array_like\n",
      "     |          Size of each query group of training data. Should have as many\n",
      "     |          elements as the query groups in the training data\n",
      "     |      sample_weight : array_like\n",
      "     |          Query group weights\n",
      "     |      \n",
      "     |          .. note:: Weights are per-group for ranking tasks\n",
      "     |      \n",
      "     |              In ranking task, one weight is assigned to each query group\n",
      "     |              (not each data point). This is because we only care about the\n",
      "     |              relative ordering of data points within each group, so it\n",
      "     |              doesn't make sense to assign weights to individual data points.\n",
      "     |      \n",
      "     |      base_margin : array_like\n",
      "     |          Global bias for each instance.\n",
      "     |      eval_set : list, optional\n",
      "     |          A list of (X, y) tuple pairs to use as validation sets, for which\n",
      "     |          metrics will be computed.\n",
      "     |          Validation metrics will help us track the performance of the model.\n",
      "     |      sample_weight_eval_set : list, optional\n",
      "     |          A list of the form [L_1, L_2, ..., L_n], where each L_i is a list of\n",
      "     |          group weights on the i-th validation set.\n",
      "     |      \n",
      "     |          .. note:: Weights are per-group for ranking tasks\n",
      "     |      \n",
      "     |              In ranking task, one weight is assigned to each query group (not each\n",
      "     |              data point). This is because we only care about the relative ordering of\n",
      "     |              data points within each group, so it doesn't make sense to assign\n",
      "     |              weights to individual data points.\n",
      "     |      \n",
      "     |      eval_group : list of arrays, optional\n",
      "     |          A list in which ``eval_group[i]`` is the list containing the sizes of all\n",
      "     |          query groups in the ``i``-th pair in **eval_set**.\n",
      "     |      eval_metric : str, list of str, optional\n",
      "     |          If a str, should be a built-in evaluation metric to use. See\n",
      "     |          doc/parameter.rst.\n",
      "     |          If a list of str, should be the list of multiple built-in evaluation metrics\n",
      "     |          to use. The custom evaluation metric is not yet supported for the ranker.\n",
      "     |      early_stopping_rounds : int\n",
      "     |          Activates early stopping. Validation metric needs to improve at least once in\n",
      "     |          every **early_stopping_rounds** round(s) to continue training.\n",
      "     |          Requires at least one item in **eval_set**.\n",
      "     |          The method returns the model from the last iteration (not the best one).\n",
      "     |          If there's more than one item in **eval_set**, the last entry will be used\n",
      "     |          for early stopping.\n",
      "     |          If there's more than one metric in **eval_metric**, the last metric\n",
      "     |          will be used for early stopping.\n",
      "     |          If early stopping occurs, the model will have three additional\n",
      "     |          fields: ``clf.best_score``, ``clf.best_iteration`` and\n",
      "     |          ``clf.best_ntree_limit``.\n",
      "     |      verbose : bool\n",
      "     |          If `verbose` and an evaluation set is used, writes the evaluation\n",
      "     |          metric measured on the validation set to stderr.\n",
      "     |      xgb_model : str\n",
      "     |          file name of stored XGBoost model or 'Booster' instance XGBoost\n",
      "     |          model to be loaded before training (allows training continuation).\n",
      "     |      feature_weights: array_like\n",
      "     |          Weight for each feature, defines the probability of each feature being\n",
      "     |          selected when colsample is being used.  All values must be greater than 0,\n",
      "     |          otherwise a `ValueError` is thrown.  Only available for `hist`, `gpu_hist` and\n",
      "     |          `exact` tree methods.\n",
      "     |      callbacks : list of callback functions\n",
      "     |          List of callback functions that are applied at end of each\n",
      "     |          iteration.  It is possible to use predefined callbacks by using\n",
      "     |          :ref:`callback_api`.  Example:\n",
      "     |      \n",
      "     |          .. code-block:: python\n",
      "     |      \n",
      "     |              callbacks = [xgb.callback.EarlyStopping(rounds=early_stopping_rounds,\n",
      "     |                                                      save_best=True)]\n",
      "     |  \n",
      "     |  predict(self, data, output_margin=False, ntree_limit=0, validate_features=True, base_margin=None)\n",
      "     |      Predict with `data`.\n",
      "     |      \n",
      "     |      .. note:: This function is not thread safe.\n",
      "     |      \n",
      "     |        For each booster object, predict can only be called from one thread.\n",
      "     |        If you want to run prediction using multiple thread, call ``xgb.copy()`` to make copies\n",
      "     |        of model object and then call ``predict()``.\n",
      "     |      \n",
      "     |        .. code-block:: python\n",
      "     |      \n",
      "     |          preds = bst.predict(dtest, ntree_limit=num_round)\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : numpy.array/scipy.sparse\n",
      "     |          Data to predict with\n",
      "     |      output_margin : bool\n",
      "     |          Whether to output the raw untransformed margin value.\n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to best_ntree_limit if defined\n",
      "     |          (i.e. it has been trained with early stopping), otherwise 0 (use all trees).\n",
      "     |      validate_features : bool\n",
      "     |          When this is True, validate that the Booster's and data's feature_names are identical.\n",
      "     |          Otherwise, it is assumed that the feature_names are the same.\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      prediction : numpy array\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from XGBModel:\n",
      "     |  \n",
      "     |  apply(self, X, ntree_limit=0)\n",
      "     |      Return the predicted leaf every tree for each sample.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like, shape=[n_samples, n_features]\n",
      "     |          Input features matrix.\n",
      "     |      \n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to 0 (use all trees).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array_like, shape=[n_samples, n_trees]\n",
      "     |          For each datapoint x in X and for each tree, return the index of the\n",
      "     |          leaf x ends up in. Leaves are numbered within\n",
      "     |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
      "     |  \n",
      "     |  evals_result(self)\n",
      "     |      Return the evaluation results.\n",
      "     |      \n",
      "     |      If **eval_set** is passed to the `fit` function, you can call\n",
      "     |      ``evals_result()`` to get evaluation results for all passed **eval_sets**.\n",
      "     |      When **eval_metric** is also passed to the `fit` function, the\n",
      "     |      **evals_result** will contain the **eval_metrics** passed to the `fit` function.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      evals_result : dictionary\n",
      "     |      \n",
      "     |      Example\n",
      "     |      -------\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          param_dist = {'objective':'binary:logistic', 'n_estimators':2}\n",
      "     |      \n",
      "     |          clf = xgb.XGBModel(**param_dist)\n",
      "     |      \n",
      "     |          clf.fit(X_train, y_train,\n",
      "     |                  eval_set=[(X_train, y_train), (X_test, y_test)],\n",
      "     |                  eval_metric='logloss',\n",
      "     |                  verbose=True)\n",
      "     |      \n",
      "     |          evals_result = clf.evals_result()\n",
      "     |      \n",
      "     |      The variable **evals_result** will contain:\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
      "     |          'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
      "     |  \n",
      "     |  get_booster(self)\n",
      "     |      Get the underlying xgboost Booster of this model.\n",
      "     |      \n",
      "     |      This will raise an exception when fit was not called\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      booster : a xgboost booster of underlying model\n",
      "     |  \n",
      "     |  get_num_boosting_rounds(self)\n",
      "     |      Gets the number of xgboost boosting rounds.\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters.\n",
      "     |  \n",
      "     |  get_xgb_params(self)\n",
      "     |      Get xgboost specific parameters.\n",
      "     |  \n",
      "     |  load_model(self, fname)\n",
      "     |      Load the model from a file.\n",
      "     |      \n",
      "     |      The model is loaded from an XGBoost internal format which is universal\n",
      "     |      among the various XGBoost interfaces. Auxiliary attributes of the\n",
      "     |      Python Booster object (such as feature names) will not be loaded.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string\n",
      "     |          Input file name.\n",
      "     |  \n",
      "     |  save_model(self, fname: str)\n",
      "     |      Save the model to a file.\n",
      "     |      \n",
      "     |      The model is saved in an XGBoost internal format which is universal\n",
      "     |      among the various XGBoost interfaces. Auxiliary attributes of the\n",
      "     |      Python Booster object (such as feature names) will not be saved.\n",
      "     |      \n",
      "     |        .. note::\n",
      "     |      \n",
      "     |          See:\n",
      "     |      \n",
      "     |          https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string\n",
      "     |          Output file name\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.  Modification of the sklearn method to\n",
      "     |      allow unknown kwargs. This allows using the full range of xgboost\n",
      "     |      parameters that are not defined as member variables in sklearn grid\n",
      "     |      search.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from XGBModel:\n",
      "     |  \n",
      "     |  coef_\n",
      "     |      Coefficients property\n",
      "     |      \n",
      "     |      .. note:: Coefficients are defined only for linear learners\n",
      "     |      \n",
      "     |          Coefficients are only defined when the linear model is chosen as\n",
      "     |          base learner (`booster=gblinear`). It is not defined for other base\n",
      "     |          learner types, such as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      coef_ : array of shape ``[n_features]`` or ``[n_classes, n_features]``\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Feature importances property\n",
      "     |      \n",
      "     |      .. note:: Feature importance is defined only for tree boosters\n",
      "     |      \n",
      "     |          Feature importance is only defined when the decision tree model is chosen as base\n",
      "     |          learner (`booster=gbtree`). It is not defined for other base learner types, such\n",
      "     |          as linear learners (`booster=gblinear`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : array of shape ``[n_features]``\n",
      "     |  \n",
      "     |  intercept_\n",
      "     |      Intercept (bias) property\n",
      "     |      \n",
      "     |      .. note:: Intercept is defined only for linear learners\n",
      "     |      \n",
      "     |          Intercept (bias) is only defined when the linear model is chosen as base\n",
      "     |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      "     |          as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      intercept_ : array of shape ``(1,)`` or ``[n_classes]``\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class XGBRegressor(XGBModel, sklearn.base.RegressorMixin)\n",
      "     |  XGBRegressor(*, objective='reg:squarederror', **kwargs)\n",
      "     |  \n",
      "     |  Implementation of the scikit-learn API for XGBoost regression.\n",
      "     |  \n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  \n",
      "     |      n_estimators : int\n",
      "     |          Number of gradient boosted trees.  Equivalent to number of boosting\n",
      "     |          rounds.\n",
      "     |  \n",
      "     |      max_depth : int\n",
      "     |          Maximum tree depth for base learners.\n",
      "     |      learning_rate : float\n",
      "     |          Boosting learning rate (xgb's \"eta\")\n",
      "     |      verbosity : int\n",
      "     |          The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
      "     |      objective : string or callable\n",
      "     |          Specify the learning task and the corresponding learning objective or\n",
      "     |          a custom objective function to be used (see note below).\n",
      "     |      booster: string\n",
      "     |          Specify which booster to use: gbtree, gblinear or dart.\n",
      "     |      tree_method: string\n",
      "     |          Specify which tree method to use.  Default to auto.  If this parameter\n",
      "     |          is set to default, XGBoost will choose the most conservative option\n",
      "     |          available.  It's recommended to study this option from parameters\n",
      "     |          document.\n",
      "     |      n_jobs : int\n",
      "     |          Number of parallel threads used to run xgboost.  When used with other Scikit-Learn\n",
      "     |          algorithms like grid search, you may choose which algorithm to parallelize and\n",
      "     |          balance the threads.  Creating thread contention will significantly slow dowm both\n",
      "     |          algorithms.\n",
      "     |      gamma : float\n",
      "     |          Minimum loss reduction required to make a further partition on a leaf\n",
      "     |          node of the tree.\n",
      "     |      min_child_weight : float\n",
      "     |          Minimum sum of instance weight(hessian) needed in a child.\n",
      "     |      max_delta_step : int\n",
      "     |          Maximum delta step we allow each tree's weight estimation to be.\n",
      "     |      subsample : float\n",
      "     |          Subsample ratio of the training instance.\n",
      "     |      colsample_bytree : float\n",
      "     |          Subsample ratio of columns when constructing each tree.\n",
      "     |      colsample_bylevel : float\n",
      "     |          Subsample ratio of columns for each level.\n",
      "     |      colsample_bynode : float\n",
      "     |          Subsample ratio of columns for each split.\n",
      "     |      reg_alpha : float (xgb's alpha)\n",
      "     |          L1 regularization term on weights\n",
      "     |      reg_lambda : float (xgb's lambda)\n",
      "     |          L2 regularization term on weights\n",
      "     |      scale_pos_weight : float\n",
      "     |          Balancing of positive and negative weights.\n",
      "     |      base_score:\n",
      "     |          The initial prediction score of all instances, global bias.\n",
      "     |      random_state : int\n",
      "     |          Random number seed.\n",
      "     |  \n",
      "     |          .. note::\n",
      "     |  \n",
      "     |             Using gblinear booster with shotgun updater is nondeterministic as\n",
      "     |             it uses Hogwild algorithm.\n",
      "     |  \n",
      "     |      missing : float, default np.nan\n",
      "     |          Value in the data which needs to be present as a missing value.\n",
      "     |      num_parallel_tree: int\n",
      "     |          Used for boosting random forest.\n",
      "     |      monotone_constraints : str\n",
      "     |          Constraint of variable monotonicity.  See tutorial for more\n",
      "     |          information.\n",
      "     |      interaction_constraints : str\n",
      "     |          Constraints for interaction representing permitted interactions.  The\n",
      "     |          constraints must be specified in the form of a nest list, e.g. [[0, 1],\n",
      "     |          [2, 3, 4]], where each inner list is a group of indices of features\n",
      "     |          that are allowed to interact with each other.  See tutorial for more\n",
      "     |          information\n",
      "     |      importance_type: string, default \"gain\"\n",
      "     |          The feature importance type for the feature_importances\\_ property:\n",
      "     |          either \"gain\", \"weight\", \"cover\", \"total_gain\" or \"total_cover\".\n",
      "     |  \n",
      "     |      \\*\\*kwargs : dict, optional\n",
      "     |          Keyword arguments for XGBoost Booster object.  Full documentation of\n",
      "     |          parameters can be found here:\n",
      "     |          https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst.\n",
      "     |          Attempting to set a parameter via the constructor args and \\*\\*kwargs\n",
      "     |          dict simultaneously will result in a TypeError.\n",
      "     |  \n",
      "     |          .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
      "     |  \n",
      "     |              \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee\n",
      "     |              that parameters passed via this argument will interact properly\n",
      "     |              with scikit-learn.\n",
      "     |  \n",
      "     |          .. note::  Custom objective function\n",
      "     |  \n",
      "     |              A custom objective function can be provided for the ``objective``\n",
      "     |              parameter. In this case, it should have the signature\n",
      "     |              ``objective(y_true, y_pred) -> grad, hess``:\n",
      "     |  \n",
      "     |              y_true: array_like of shape [n_samples]\n",
      "     |                  The target values\n",
      "     |              y_pred: array_like of shape [n_samples]\n",
      "     |                  The predicted values\n",
      "     |  \n",
      "     |              grad: array_like of shape [n_samples]\n",
      "     |                  The value of the gradient for each sample point.\n",
      "     |              hess: array_like of shape [n_samples]\n",
      "     |                  The value of the second derivative for each sample point\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      XGBRegressor\n",
      "     |      XGBModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, objective='reg:squarederror', **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from XGBModel:\n",
      "     |  \n",
      "     |  apply(self, X, ntree_limit=0)\n",
      "     |      Return the predicted leaf every tree for each sample.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like, shape=[n_samples, n_features]\n",
      "     |          Input features matrix.\n",
      "     |      \n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to 0 (use all trees).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array_like, shape=[n_samples, n_trees]\n",
      "     |          For each datapoint x in X and for each tree, return the index of the\n",
      "     |          leaf x ends up in. Leaves are numbered within\n",
      "     |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
      "     |  \n",
      "     |  evals_result(self)\n",
      "     |      Return the evaluation results.\n",
      "     |      \n",
      "     |      If **eval_set** is passed to the `fit` function, you can call\n",
      "     |      ``evals_result()`` to get evaluation results for all passed **eval_sets**.\n",
      "     |      When **eval_metric** is also passed to the `fit` function, the\n",
      "     |      **evals_result** will contain the **eval_metrics** passed to the `fit` function.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      evals_result : dictionary\n",
      "     |      \n",
      "     |      Example\n",
      "     |      -------\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          param_dist = {'objective':'binary:logistic', 'n_estimators':2}\n",
      "     |      \n",
      "     |          clf = xgb.XGBModel(**param_dist)\n",
      "     |      \n",
      "     |          clf.fit(X_train, y_train,\n",
      "     |                  eval_set=[(X_train, y_train), (X_test, y_test)],\n",
      "     |                  eval_metric='logloss',\n",
      "     |                  verbose=True)\n",
      "     |      \n",
      "     |          evals_result = clf.evals_result()\n",
      "     |      \n",
      "     |      The variable **evals_result** will contain:\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
      "     |          'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
      "     |  \n",
      "     |  fit(self, X, y, *, sample_weight=None, base_margin=None, eval_set=None, eval_metric=None, early_stopping_rounds=None, verbose=True, xgb_model=None, sample_weight_eval_set=None, feature_weights=None, callbacks=None)\n",
      "     |      Fit gradient boosting model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like\n",
      "     |          Feature matrix\n",
      "     |      y : array_like\n",
      "     |          Labels\n",
      "     |      sample_weight : array_like\n",
      "     |          instance weights\n",
      "     |      base_margin : array_like\n",
      "     |          global bias for each instance.\n",
      "     |      eval_set : list, optional\n",
      "     |          A list of (X, y) tuple pairs to use as validation sets, for which\n",
      "     |          metrics will be computed.\n",
      "     |          Validation metrics will help us track the performance of the model.\n",
      "     |      eval_metric : str, list of str, or callable, optional\n",
      "     |          If a str, should be a built-in evaluation metric to use. See\n",
      "     |          doc/parameter.rst.\n",
      "     |          If a list of str, should be the list of multiple built-in evaluation metrics\n",
      "     |          to use.\n",
      "     |          If callable, a custom evaluation metric. The call\n",
      "     |          signature is ``func(y_predicted, y_true)`` where ``y_true`` will be a\n",
      "     |          DMatrix object such that you may need to call the ``get_label``\n",
      "     |          method. It must return a str, value pair where the str is a name\n",
      "     |          for the evaluation and value is the value of the evaluation\n",
      "     |          function. The callable custom objective is always minimized.\n",
      "     |      early_stopping_rounds : int\n",
      "     |          Activates early stopping. Validation metric needs to improve at least once in\n",
      "     |          every **early_stopping_rounds** round(s) to continue training.\n",
      "     |          Requires at least one item in **eval_set**.\n",
      "     |          The method returns the model from the last iteration (not the best one).\n",
      "     |          If there's more than one item in **eval_set**, the last entry will be used\n",
      "     |          for early stopping.\n",
      "     |          If there's more than one metric in **eval_metric**, the last metric will be\n",
      "     |          used for early stopping.\n",
      "     |          If early stopping occurs, the model will have three additional fields:\n",
      "     |          ``clf.best_score``, ``clf.best_iteration`` and ``clf.best_ntree_limit``.\n",
      "     |      verbose : bool\n",
      "     |          If `verbose` and an evaluation set is used, writes the evaluation\n",
      "     |          metric measured on the validation set to stderr.\n",
      "     |      xgb_model : str\n",
      "     |          file name of stored XGBoost model or 'Booster' instance XGBoost model to be\n",
      "     |          loaded before training (allows training continuation).\n",
      "     |      sample_weight_eval_set : list, optional\n",
      "     |          A list of the form [L_1, L_2, ..., L_n], where each L_i is a list of\n",
      "     |          instance weights on the i-th validation set.\n",
      "     |      feature_weights: array_like\n",
      "     |          Weight for each feature, defines the probability of each feature being\n",
      "     |          selected when colsample is being used.  All values must be greater than 0,\n",
      "     |          otherwise a `ValueError` is thrown.  Only available for `hist`, `gpu_hist` and\n",
      "     |          `exact` tree methods.\n",
      "     |      callbacks : list of callback functions\n",
      "     |          List of callback functions that are applied at end of each iteration.\n",
      "     |          It is possible to use predefined callbacks by using :ref:`callback_api`.\n",
      "     |          Example:\n",
      "     |      \n",
      "     |          .. code-block:: python\n",
      "     |      \n",
      "     |              callbacks = [xgb.callback.EarlyStopping(rounds=early_stopping_rounds,\n",
      "     |                                                      save_best=True)]\n",
      "     |  \n",
      "     |  get_booster(self)\n",
      "     |      Get the underlying xgboost Booster of this model.\n",
      "     |      \n",
      "     |      This will raise an exception when fit was not called\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      booster : a xgboost booster of underlying model\n",
      "     |  \n",
      "     |  get_num_boosting_rounds(self)\n",
      "     |      Gets the number of xgboost boosting rounds.\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters.\n",
      "     |  \n",
      "     |  get_xgb_params(self)\n",
      "     |      Get xgboost specific parameters.\n",
      "     |  \n",
      "     |  load_model(self, fname)\n",
      "     |      Load the model from a file.\n",
      "     |      \n",
      "     |      The model is loaded from an XGBoost internal format which is universal\n",
      "     |      among the various XGBoost interfaces. Auxiliary attributes of the\n",
      "     |      Python Booster object (such as feature names) will not be loaded.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string\n",
      "     |          Input file name.\n",
      "     |  \n",
      "     |  predict(self, data, output_margin=False, ntree_limit=None, validate_features=True, base_margin=None)\n",
      "     |      Predict with `data`.\n",
      "     |      \n",
      "     |      .. note:: This function is not thread safe.\n",
      "     |      \n",
      "     |        For each booster object, predict can only be called from one thread.\n",
      "     |        If you want to run prediction using multiple thread, call ``xgb.copy()`` to make copies\n",
      "     |        of model object and then call ``predict()``.\n",
      "     |      \n",
      "     |        .. code-block:: python\n",
      "     |      \n",
      "     |          preds = bst.predict(dtest, ntree_limit=num_round)\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : numpy.array/scipy.sparse\n",
      "     |          Data to predict with\n",
      "     |      output_margin : bool\n",
      "     |          Whether to output the raw untransformed margin value.\n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to best_ntree_limit if defined\n",
      "     |          (i.e. it has been trained with early stopping), otherwise 0 (use all trees).\n",
      "     |      validate_features : bool\n",
      "     |          When this is True, validate that the Booster's and data's feature_names are identical.\n",
      "     |          Otherwise, it is assumed that the feature_names are the same.\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      prediction : numpy array\n",
      "     |  \n",
      "     |  save_model(self, fname: str)\n",
      "     |      Save the model to a file.\n",
      "     |      \n",
      "     |      The model is saved in an XGBoost internal format which is universal\n",
      "     |      among the various XGBoost interfaces. Auxiliary attributes of the\n",
      "     |      Python Booster object (such as feature names) will not be saved.\n",
      "     |      \n",
      "     |        .. note::\n",
      "     |      \n",
      "     |          See:\n",
      "     |      \n",
      "     |          https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string\n",
      "     |          Output file name\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.  Modification of the sklearn method to\n",
      "     |      allow unknown kwargs. This allows using the full range of xgboost\n",
      "     |      parameters that are not defined as member variables in sklearn grid\n",
      "     |      search.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from XGBModel:\n",
      "     |  \n",
      "     |  coef_\n",
      "     |      Coefficients property\n",
      "     |      \n",
      "     |      .. note:: Coefficients are defined only for linear learners\n",
      "     |      \n",
      "     |          Coefficients are only defined when the linear model is chosen as\n",
      "     |          base learner (`booster=gblinear`). It is not defined for other base\n",
      "     |          learner types, such as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      coef_ : array of shape ``[n_features]`` or ``[n_classes, n_features]``\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Feature importances property\n",
      "     |      \n",
      "     |      .. note:: Feature importance is defined only for tree boosters\n",
      "     |      \n",
      "     |          Feature importance is only defined when the decision tree model is chosen as base\n",
      "     |          learner (`booster=gbtree`). It is not defined for other base learner types, such\n",
      "     |          as linear learners (`booster=gblinear`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : array of shape ``[n_features]``\n",
      "     |  \n",
      "     |  intercept_\n",
      "     |      Intercept (bias) property\n",
      "     |      \n",
      "     |      .. note:: Intercept is defined only for linear learners\n",
      "     |      \n",
      "     |          Intercept (bias) is only defined when the linear model is chosen as base\n",
      "     |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      "     |          as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      intercept_ : array of shape ``(1,)`` or ``[n_classes]``\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a\n",
      "     |          precomputed kernel matrix or a list of generic objects instead,\n",
      "     |          shape = (n_samples, n_samples_fitted),\n",
      "     |          where n_samples_fitted is the number of\n",
      "     |          samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The R2 score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "\n",
      "FUNCTIONS\n",
      "    xgboost_model_doc(header, items, extra_parameters=None, end_note=None)\n",
      "        Obtain documentation for Scikit-Learn wrappers\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        header: str\n",
      "           An introducion to the class.\n",
      "        items : list\n",
      "           A list of commom doc items.  Available items are:\n",
      "             - estimators: the meaning of n_estimators\n",
      "             - model: All the other parameters\n",
      "             - objective: note for customized objective\n",
      "        extra_parameters: str\n",
      "           Document for class specific parameters, placed at the head.\n",
      "        end_note: str\n",
      "           Extra notes put to the end.\n",
      "\n",
      "DATA\n",
      "    Optional = typing.Optional\n",
      "    SKLEARN_INSTALLED = True\n",
      "\n",
      "FILE\n",
      "    /home/baldeaguirre/.local/lib/python3.8/site-packages/xgboost/sklearn.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(xgb.sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "verified-niagara",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Booster in module xgboost.core:\n",
      "\n",
      "class Booster(builtins.object)\n",
      " |  Booster(params=None, cache=(), model_file=None)\n",
      " |  \n",
      " |  A Booster of XGBoost.\n",
      " |  \n",
      " |  Booster is the model of xgboost, that contains low level routines for\n",
      " |  training, prediction and evaluation.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __copy__(self)\n",
      " |  \n",
      " |  __deepcopy__(self, _)\n",
      " |      Return a copy of booster.\n",
      " |  \n",
      " |  __del__(self)\n",
      " |  \n",
      " |  __getitem__(self, val)\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __init__(self, params=None, cache=(), model_file=None)\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : dict\n",
      " |          Parameters for boosters.\n",
      " |      cache : list\n",
      " |          List of cache items.\n",
      " |      model_file : string/os.PathLike/Booster/bytearray\n",
      " |          Path to the model file if it's string or PathLike.\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  attr(self, key)\n",
      " |      Get attribute string from the Booster.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      key : str\n",
      " |          The key to get attribute from.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      value : str\n",
      " |          The attribute value of the key, returns None if attribute do not exist.\n",
      " |  \n",
      " |  attributes(self)\n",
      " |      Get attributes stored in the Booster as a dictionary.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      result : dictionary of  attribute_name: attribute_value pairs of strings.\n",
      " |          Returns an empty dict if there's no attributes.\n",
      " |  \n",
      " |  boost(self, dtrain, grad, hess)\n",
      " |      Boost the booster for one iteration, with customized gradient\n",
      " |      statistics.  Like :func:`xgboost.core.Booster.update`, this\n",
      " |      function should not be called directly by users.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dtrain : DMatrix\n",
      " |          The training DMatrix.\n",
      " |      grad : list\n",
      " |          The first order of gradient.\n",
      " |      hess : list\n",
      " |          The second order of gradient.\n",
      " |  \n",
      " |  copy(self)\n",
      " |      Copy the booster object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      booster: `Booster`\n",
      " |          a copied booster model\n",
      " |  \n",
      " |  dump_model(self, fout, fmap='', with_stats=False, dump_format='text')\n",
      " |      Dump model into a text or JSON file.  Unlike `save_model`, the\n",
      " |      output format is primarily used for visualization or interpretation,\n",
      " |      hence it's more human readable but cannot be loaded back to XGBoost.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fout : string or os.PathLike\n",
      " |          Output file name.\n",
      " |      fmap : string or os.PathLike, optional\n",
      " |          Name of the file containing feature map names.\n",
      " |      with_stats : bool, optional\n",
      " |          Controls whether the split statistics are output.\n",
      " |      dump_format : string, optional\n",
      " |          Format of model dump file. Can be 'text' or 'json'.\n",
      " |  \n",
      " |  eval(self, data, name='eval', iteration=0)\n",
      " |      Evaluate the model on mat.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : DMatrix\n",
      " |          The dmatrix storing the input.\n",
      " |      \n",
      " |      name : str, optional\n",
      " |          The name of the dataset.\n",
      " |      \n",
      " |      iteration : int, optional\n",
      " |          The current iteration number.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      result: str\n",
      " |          Evaluation result string.\n",
      " |  \n",
      " |  eval_set(self, evals, iteration=0, feval=None)\n",
      " |      Evaluate a set of data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      evals : list of tuples (DMatrix, string)\n",
      " |          List of items to be evaluated.\n",
      " |      iteration : int\n",
      " |          Current iteration.\n",
      " |      feval : function\n",
      " |          Custom evaluation function.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      result: str\n",
      " |          Evaluation result string.\n",
      " |  \n",
      " |  get_dump(self, fmap='', with_stats=False, dump_format='text')\n",
      " |      Returns the model dump as a list of strings.  Unlike `save_model`, the\n",
      " |      output format is primarily used for visualization or interpretation,\n",
      " |      hence it's more human readable but cannot be loaded back to XGBoost.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fmap : string or os.PathLike, optional\n",
      " |          Name of the file containing feature map names.\n",
      " |      with_stats : bool, optional\n",
      " |          Controls whether the split statistics are output.\n",
      " |      dump_format : string, optional\n",
      " |          Format of model dump. Can be 'text', 'json' or 'dot'.\n",
      " |  \n",
      " |  get_fscore(self, fmap='')\n",
      " |      Get feature importance of each feature.\n",
      " |      \n",
      " |      .. note:: Feature importance is defined only for tree boosters\n",
      " |      \n",
      " |          Feature importance is only defined when the decision tree model is chosen as base\n",
      " |          learner (`booster=gbtree`). It is not defined for other base learner types, such\n",
      " |          as linear learners (`booster=gblinear`).\n",
      " |      \n",
      " |      .. note:: Zero-importance features will not be included\n",
      " |      \n",
      " |         Keep in mind that this function does not include zero-importance feature, i.e.\n",
      " |         those features that have not been used in any split conditions.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fmap: str or os.PathLike (optional)\n",
      " |         The name of feature map file\n",
      " |  \n",
      " |  get_score(self, fmap='', importance_type='weight')\n",
      " |      Get feature importance of each feature.\n",
      " |      Importance type can be defined as:\n",
      " |      \n",
      " |      * 'weight': the number of times a feature is used to split the data across all trees.\n",
      " |      * 'gain': the average gain across all splits the feature is used in.\n",
      " |      * 'cover': the average coverage across all splits the feature is used in.\n",
      " |      * 'total_gain': the total gain across all splits the feature is used in.\n",
      " |      * 'total_cover': the total coverage across all splits the feature is used in.\n",
      " |      \n",
      " |      .. note:: Feature importance is defined only for tree boosters\n",
      " |      \n",
      " |          Feature importance is only defined when the decision tree model is chosen as base\n",
      " |          learner (`booster=gbtree`). It is not defined for other base learner types, such\n",
      " |          as linear learners (`booster=gblinear`).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fmap: str or os.PathLike (optional)\n",
      " |         The name of feature map file.\n",
      " |      importance_type: str, default 'weight'\n",
      " |          One of the importance types defined above.\n",
      " |  \n",
      " |  get_split_value_histogram(self, feature, fmap='', bins=None, as_pandas=True)\n",
      " |      Get split value histogram of a feature\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      feature: str\n",
      " |          The name of the feature.\n",
      " |      fmap: str or os.PathLike (optional)\n",
      " |          The name of feature map file.\n",
      " |      bin: int, default None\n",
      " |          The maximum number of bins.\n",
      " |          Number of bins equals number of unique split values n_unique,\n",
      " |          if bins == None or bins > n_unique.\n",
      " |      as_pandas: bool, default True\n",
      " |          Return pd.DataFrame when pandas is installed.\n",
      " |          If False or pandas is not installed, return numpy ndarray.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      a histogram of used splitting values for the specified feature\n",
      " |      either as numpy array or pandas DataFrame.\n",
      " |  \n",
      " |  inplace_predict(self, data, iteration_range=(0, 0), predict_type='value', missing=nan)\n",
      " |      Run prediction in-place, Unlike ``predict`` method, inplace prediction does\n",
      " |      not cache the prediction result.\n",
      " |      \n",
      " |      Calling only ``inplace_predict`` in multiple threads is safe and lock\n",
      " |      free.  But the safety does not hold when used in conjunction with other\n",
      " |      methods. E.g. you can't train the booster in one thread and perform\n",
      " |      prediction in the other.\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          booster.set_param({'predictor': 'gpu_predictor'})\n",
      " |          booster.inplace_predict(cupy_array)\n",
      " |      \n",
      " |          booster.set_param({'predictor': 'cpu_predictor})\n",
      " |          booster.inplace_predict(numpy_array)\n",
      " |      \n",
      " |      .. versionadded:: 1.1.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : numpy.ndarray/scipy.sparse.csr_matrix/cupy.ndarray/\n",
      " |             cudf.DataFrame/pd.DataFrame\n",
      " |          The input data, must not be a view for numpy array.  Set\n",
      " |          ``predictor`` to ``gpu_predictor`` for running prediction on CuPy\n",
      " |          array or CuDF DataFrame.\n",
      " |      iteration_range : tuple\n",
      " |          Specifies which layer of trees are used in prediction.  For\n",
      " |          example, if a random forest is trained with 100 rounds.  Specifying\n",
      " |          `iteration_range=(10, 20)`, then only the forests built during [10,\n",
      " |          20) (open set) rounds are used in this prediction.\n",
      " |      predict_type : str\n",
      " |          * `value` Output model prediction values.\n",
      " |          * `margin` Output the raw untransformed margin value.\n",
      " |      missing : float\n",
      " |          Value in the input data which needs to be present as a missing\n",
      " |          value.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction : numpy.ndarray/cupy.ndarray\n",
      " |          The prediction result.  When input data is on GPU, prediction\n",
      " |          result is stored in a cupy array.\n",
      " |  \n",
      " |  load_config(self, config)\n",
      " |      Load configuration returned by `save_config`.\n",
      " |      \n",
      " |      .. versionadded:: 1.0.0\n",
      " |  \n",
      " |  load_model(self, fname)\n",
      " |      Load the model from a file or bytearray. Path to file can be local\n",
      " |      or as an URI.\n",
      " |      \n",
      " |      The model is loaded from XGBoost format which is universal among the\n",
      " |      various XGBoost interfaces. Auxiliary attributes of the Python Booster\n",
      " |      object (such as feature_names) will not be loaded.  See:\n",
      " |      \n",
      " |        https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
      " |      \n",
      " |      for more info.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : string, os.PathLike, or a memory buffer\n",
      " |          Input file name or memory buffer(see also save_raw)\n",
      " |  \n",
      " |  load_rabit_checkpoint(self)\n",
      " |      Initialize the model by load from rabit checkpoint.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      version: integer\n",
      " |          The version number of the model.\n",
      " |  \n",
      " |  predict(self, data, output_margin=False, ntree_limit=0, pred_leaf=False, pred_contribs=False, approx_contribs=False, pred_interactions=False, validate_features=True, training=False)\n",
      " |      Predict with data.\n",
      " |      \n",
      " |      .. note:: This function is not thread safe except for ``gbtree``\n",
      " |                booster.\n",
      " |      \n",
      " |        For ``gbtree`` booster, the thread safety is guaranteed by locks.\n",
      " |        For lock free prediction use ``inplace_predict`` instead.  Also, the\n",
      " |        safety does not hold when used in conjunction with other methods.\n",
      " |      \n",
      " |        When using booster other than ``gbtree``, predict can only be called\n",
      " |        from one thread.  If you want to run prediction using multiple\n",
      " |        thread, call ``bst.copy()`` to make copies of model object and then\n",
      " |        call ``predict()``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : DMatrix\n",
      " |          The dmatrix storing the input.\n",
      " |      \n",
      " |      output_margin : bool\n",
      " |          Whether to output the raw untransformed margin value.\n",
      " |      \n",
      " |      ntree_limit : int\n",
      " |          Limit number of trees in the prediction; defaults to 0 (use all\n",
      " |          trees).\n",
      " |      \n",
      " |      pred_leaf : bool\n",
      " |          When this option is on, the output will be a matrix of (nsample,\n",
      " |          ntrees) with each record indicating the predicted leaf index of\n",
      " |          each sample in each tree.  Note that the leaf index of a tree is\n",
      " |          unique per tree, so you may find leaf 1 in both tree 1 and tree 0.\n",
      " |      \n",
      " |      pred_contribs : bool\n",
      " |          When this is True the output will be a matrix of size (nsample,\n",
      " |          nfeats + 1) with each record indicating the feature contributions\n",
      " |          (SHAP values) for that prediction. The sum of all feature\n",
      " |          contributions is equal to the raw untransformed margin value of the\n",
      " |          prediction. Note the final column is the bias term.\n",
      " |      \n",
      " |      approx_contribs : bool\n",
      " |          Approximate the contributions of each feature\n",
      " |      \n",
      " |      pred_interactions : bool\n",
      " |          When this is True the output will be a matrix of size (nsample,\n",
      " |          nfeats + 1, nfeats + 1) indicating the SHAP interaction values for\n",
      " |          each pair of features. The sum of each row (or column) of the\n",
      " |          interaction values equals the corresponding SHAP value (from\n",
      " |          pred_contribs), and the sum of the entire matrix equals the raw\n",
      " |          untransformed margin value of the prediction. Note the last row and\n",
      " |          column correspond to the bias term.\n",
      " |      \n",
      " |      validate_features : bool\n",
      " |          When this is True, validate that the Booster's and data's\n",
      " |          feature_names are identical.  Otherwise, it is assumed that the\n",
      " |          feature_names are the same.\n",
      " |      \n",
      " |      training : bool\n",
      " |          Whether the prediction value is used for training.  This can effect\n",
      " |          `dart` booster, which performs dropouts during training iterations.\n",
      " |      \n",
      " |          .. versionadded:: 1.0.0\n",
      " |      \n",
      " |      .. note:: Using ``predict()`` with DART booster\n",
      " |      \n",
      " |        If the booster object is DART type, ``predict()`` will not perform\n",
      " |        dropouts, i.e. all the trees will be evaluated.  If you want to\n",
      " |        obtain result with dropouts, provide `training=True`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction : numpy array\n",
      " |  \n",
      " |  save_config(self)\n",
      " |      Output internal parameter configuration of Booster as a JSON\n",
      " |      string.\n",
      " |      \n",
      " |      .. versionadded:: 1.0.0\n",
      " |  \n",
      " |  save_model(self, fname)\n",
      " |      Save the model to a file.\n",
      " |      \n",
      " |      The model is saved in an XGBoost internal format which is universal\n",
      " |      among the various XGBoost interfaces. Auxiliary attributes of the\n",
      " |      Python Booster object (such as feature_names) will not be saved.  See:\n",
      " |      \n",
      " |        https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
      " |      \n",
      " |      for more info.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : string or os.PathLike\n",
      " |          Output file name\n",
      " |  \n",
      " |  save_rabit_checkpoint(self)\n",
      " |      Save the current booster to rabit checkpoint.\n",
      " |  \n",
      " |  save_raw(self)\n",
      " |      Save the model to a in memory buffer representation instead of file.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      a in memory buffer representation of the model\n",
      " |  \n",
      " |  set_attr(self, **kwargs)\n",
      " |      Set the attribute of the Booster.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **kwargs\n",
      " |          The attributes to set. Setting a value to None deletes an attribute.\n",
      " |  \n",
      " |  set_param(self, params, value=None)\n",
      " |      Set parameters into the Booster.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params: dict/list/str\n",
      " |         list of key,value pairs, dict of key to value or simply str key\n",
      " |      value: optional\n",
      " |         value of the specified parameter, when params is str key\n",
      " |  \n",
      " |  trees_to_dataframe(self, fmap='')\n",
      " |      Parse a boosted tree model text dump into a pandas DataFrame structure.\n",
      " |      \n",
      " |      This feature is only defined when the decision tree model is chosen as base\n",
      " |      learner (`booster in {gbtree, dart}`). It is not defined for other base learner\n",
      " |      types, such as linear learners (`booster=gblinear`).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fmap: str or os.PathLike (optional)\n",
      " |         The name of feature map file.\n",
      " |  \n",
      " |  update(self, dtrain, iteration, fobj=None)\n",
      " |      Update for one iteration, with objective function calculated\n",
      " |      internally.  This function should not be called directly by users.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dtrain : DMatrix\n",
      " |          Training data.\n",
      " |      iteration : int\n",
      " |          Current iteration number.\n",
      " |      fobj : function\n",
      " |          Customized objective function.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  feature_names = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(xgb.Booster)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
